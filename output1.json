{
  "model": "lmstudio-community/Qwen3-0.6B-MLX-bf16",
  "dataset": "/Users/huangzhe/Documents/GitHub/A-mem-baseline/data/locomo10.json",
  "total_questions": 199,
  "category_distribution": {
    "2": 37,
    "3": 13,
    "1": 32,
    "4": 70,
    "5": 47
  },
  "aggregate_metrics": {
    "overall": {
      "exact_match": {
        "mean": 0.08040201005025126,
        "std": 0.27260034607976563,
        "median": 0,
        "min": 0,
        "max": 1,
        "count": 199
      },
      "f1": {
        "mean": 0.3083664489647829,
        "std": 0.33196538229159267,
        "median": 0.2222222222222222,
        "min": 0.0,
        "max": 1.0,
        "count": 199
      },
      "rouge1_f": {
        "mean": 0.3257097933528585,
        "std": 0.3283141242672318,
        "median": 0.23529411764705882,
        "min": 0.0,
        "max": 1.0,
        "count": 199
      },
      "rouge2_f": {
        "mean": 0.20053160567116,
        "std": 0.3344131493391752,
        "median": 0.0,
        "min": 0.0,
        "max": 1.0,
        "count": 199
      },
      "rougeL_f": {
        "mean": 0.31744600948463764,
        "std": 0.32980696190725683,
        "median": 0.2222222222222222,
        "min": 0.0,
        "max": 1.0,
        "count": 199
      },
      "bleu1": {
        "mean": 0.2561180732737921,
        "std": 0.3030878143714809,
        "median": 0.15159143965003283,
        "min": 0,
        "max": 1.0,
        "count": 199
      },
      "bleu2": {
        "mean": 0.1871998752479697,
        "std": 0.2935476003532364,
        "median": 0.048795003647426664,
        "min": 0,
        "max": 1.0,
        "count": 199
      },
      "bleu3": {
        "mean": 0.15380396304326177,
        "std": 0.27266641433218525,
        "median": 0.037468614372419724,
        "min": 0,
        "max": 1.0,
        "count": 199
      },
      "bleu4": {
        "mean": 0.12247758865249876,
        "std": 0.2323743762042775,
        "median": 0.029950417327249696,
        "min": 0,
        "max": 1.0,
        "count": 199
      },
      "bert_precision": {
        "mean": 0.8808941742283615,
        "std": 0.0626399509670385,
        "median": 0.8717861175537109,
        "min": 0.7127465605735779,
        "max": 1.000000238418579,
        "count": 199
      },
      "bert_recall": {
        "mean": 0.8898643626639592,
        "std": 0.05722929008435517,
        "median": 0.8775244355201721,
        "min": 0.7505234479904175,
        "max": 1.000000238418579,
        "count": 199
      },
      "bert_f1": {
        "mean": 0.8848995880864973,
        "std": 0.056860665205079694,
        "median": 0.874138355255127,
        "min": 0.7398236989974976,
        "max": 1.000000238418579,
        "count": 199
      },
      "meteor": {
        "mean": 0.2485596898255304,
        "std": 0.31372566626657544,
        "median": 0.125,
        "min": 0.0,
        "max": 0.9993141289437586,
        "count": 199
      },
      "sbert_similarity": {
        "mean": 0.5252747202067444,
        "std": 0.28813662780820987,
        "median": 0.48991337418556213,
        "min": -0.03565456345677376,
        "max": 1.0000001192092896,
        "count": 199
      }
    },
    "category_1": {
      "exact_match": {
        "mean": 0,
        "std": 0.0,
        "median": 0.0,
        "min": 0,
        "max": 0,
        "count": 32
      },
      "f1": {
        "mean": 0.11876394678118816,
        "std": 0.1434648492563392,
        "median": 0.057142857142857134,
        "min": 0.0,
        "max": 0.5,
        "count": 32
      },
      "rouge1_f": {
        "mean": 0.1290983880237382,
        "std": 0.13443088882188886,
        "median": 0.12310606060606061,
        "min": 0.0,
        "max": 0.5,
        "count": 32
      },
      "rouge2_f": {
        "mean": 0.019785285356406047,
        "std": 0.04786781062103235,
        "median": 0.0,
        "min": 0.0,
        "max": 0.2,
        "count": 32
      },
      "rougeL_f": {
        "mean": 0.12055169896718071,
        "std": 0.13358705774070068,
        "median": 0.10526315789473685,
        "min": 0.0,
        "max": 0.5,
        "count": 32
      },
      "bleu1": {
        "mean": 0.0951229595538545,
        "std": 0.11300130216878831,
        "median": 0.07964714206690682,
        "min": 0,
        "max": 0.40219202762138356,
        "count": 32
      },
      "bleu2": {
        "mean": 0.0411278918713011,
        "std": 0.058738507387087115,
        "median": 0.020700445874801132,
        "min": 0,
        "max": 0.259613837492502,
        "count": 32
      },
      "bleu3": {
        "mean": 0.02732171061247008,
        "std": 0.03790498872314761,
        "median": 0.013293929972574,
        "min": 0,
        "max": 0.12111838948674812,
        "count": 32
      },
      "bleu4": {
        "mean": 0.02156194931540012,
        "std": 0.03138026014724103,
        "median": 0.010024050086992684,
        "min": 0,
        "max": 0.11362193664674995,
        "count": 32
      },
      "bert_precision": {
        "mean": 0.8469133507460356,
        "std": 0.02674357251138236,
        "median": 0.8470539450645447,
        "min": 0.7935880422592163,
        "max": 0.903893232345581,
        "count": 32
      },
      "bert_recall": {
        "mean": 0.8559469506144524,
        "std": 0.02965278959647891,
        "median": 0.8588534891605377,
        "min": 0.7767043113708496,
        "max": 0.9215748906135559,
        "count": 32
      },
      "bert_f1": {
        "mean": 0.851086737588048,
        "std": 0.022794150341415485,
        "median": 0.8483387231826782,
        "min": 0.8005561828613281,
        "max": 0.8967539668083191,
        "count": 32
      },
      "meteor": {
        "mean": 0.06897350448899499,
        "std": 0.10533330638093186,
        "median": 0.0,
        "min": 0.0,
        "max": 0.3758169934640523,
        "count": 32
      },
      "sbert_similarity": {
        "mean": 0.37252915115095675,
        "std": 0.14211681731974932,
        "median": 0.3708852231502533,
        "min": 0.10085391253232956,
        "max": 0.7062749862670898,
        "count": 32
      }
    },
    "category_2": {
      "exact_match": {
        "mean": 0,
        "std": 0.0,
        "median": 0,
        "min": 0,
        "max": 0,
        "count": 37
      },
      "f1": {
        "mean": 0.288559235927657,
        "std": 0.237739113547297,
        "median": 0.25,
        "min": 0.0,
        "max": 0.6666666666666666,
        "count": 37
      },
      "rouge1_f": {
        "mean": 0.3032637032637033,
        "std": 0.2204603591627105,
        "median": 0.28571428571428575,
        "min": 0.0,
        "max": 0.6666666666666666,
        "count": 37
      },
      "rouge2_f": {
        "mean": 0.1290979290979291,
        "std": 0.19390133252139066,
        "median": 0.0,
        "min": 0.0,
        "max": 0.5714285714285715,
        "count": 37
      },
      "rougeL_f": {
        "mean": 0.3032637032637033,
        "std": 0.2204603591627105,
        "median": 0.28571428571428575,
        "min": 0.0,
        "max": 0.6666666666666666,
        "count": 37
      },
      "bleu1": {
        "mean": 0.20167441541710304,
        "std": 0.16910112095408572,
        "median": 0.16666666666666669,
        "min": 0,
        "max": 0.5,
        "count": 37
      },
      "bleu2": {
        "mean": 0.08788784843039417,
        "std": 0.09706700132957229,
        "median": 0.05773502691896258,
        "min": 0,
        "max": 0.316227766016838,
        "count": 37
      },
      "bleu3": {
        "mean": 0.05228856131102755,
        "std": 0.0460085612077835,
        "median": 0.04506819749679463,
        "min": 0,
        "max": 0.14283448174223617,
        "count": 37
      },
      "bleu4": {
        "mean": 0.043995775481064245,
        "std": 0.036514454281589274,
        "median": 0.039281465090051315,
        "min": 0,
        "max": 0.11404605374835301,
        "count": 37
      },
      "bert_precision": {
        "mean": 0.8599844526600193,
        "std": 0.07204607657978955,
        "median": 0.8706039190292358,
        "min": 0.7127465605735779,
        "max": 0.953313946723938,
        "count": 37
      },
      "bert_recall": {
        "mean": 0.8809917101988921,
        "std": 0.044723893068896896,
        "median": 0.879320502281189,
        "min": 0.7505234479904175,
        "max": 0.9713683128356934,
        "count": 37
      },
      "bert_f1": {
        "mean": 0.8692914827449901,
        "std": 0.052339493262769335,
        "median": 0.8742544054985046,
        "min": 0.7398236989974976,
        "max": 0.9546660780906677,
        "count": 37
      },
      "meteor": {
        "mean": 0.10861377955791275,
        "std": 0.10973880596565125,
        "median": 0.08620689655172413,
        "min": 0.0,
        "max": 0.3333333333333333,
        "count": 37
      },
      "sbert_similarity": {
        "mean": 0.6986515771698307,
        "std": 0.15648971644462595,
        "median": 0.7179256677627563,
        "min": 0.2211436629295349,
        "max": 0.9388758540153503,
        "count": 37
      }
    },
    "category_3": {
      "exact_match": {
        "mean": 0,
        "std": 0.0,
        "median": 0,
        "min": 0,
        "max": 0,
        "count": 13
      },
      "f1": {
        "mean": 0.08426885754820977,
        "std": 0.14155331336289326,
        "median": 0.0,
        "min": 0.0,
        "max": 0.4444444444444445,
        "count": 13
      },
      "rouge1_f": {
        "mean": 0.11215237753699292,
        "std": 0.14546843430326276,
        "median": 0.060606060606060615,
        "min": 0.0,
        "max": 0.4444444444444445,
        "count": 13
      },
      "rouge2_f": {
        "mean": 0.010989010989010988,
        "std": 0.03962144258751636,
        "median": 0.0,
        "min": 0.0,
        "max": 0.14285714285714285,
        "count": 13
      },
      "rougeL_f": {
        "mean": 0.11215237753699292,
        "std": 0.14546843430326276,
        "median": 0.060606060606060615,
        "min": 0.0,
        "max": 0.4444444444444445,
        "count": 13
      },
      "bleu1": {
        "mean": 0.09348062264009584,
        "std": 0.11381119571090084,
        "median": 0.043478260869565216,
        "min": 0,
        "max": 0.3274923012311928,
        "count": 13
      },
      "bleu2": {
        "mean": 0.03741944756348354,
        "std": 0.05552144366429464,
        "median": 0.014058038927888336,
        "min": 0,
        "max": 0.1651445647689541,
        "count": 13
      },
      "bleu3": {
        "mean": 0.01980111143656677,
        "std": 0.025886590633946816,
        "median": 0.010263558503321437,
        "min": 0,
        "max": 0.06900656343516053,
        "count": 13
      },
      "bleu4": {
        "mean": 0.014701658510164665,
        "std": 0.018900103066160905,
        "median": 0.008282282660969604,
        "min": 0,
        "max": 0.05231223689135342,
        "count": 13
      },
      "bert_precision": {
        "mean": 0.864013291322268,
        "std": 0.0311078766564357,
        "median": 0.8665491342544556,
        "min": 0.7920843362808228,
        "max": 0.9097069501876831,
        "count": 13
      },
      "bert_recall": {
        "mean": 0.8635656375151414,
        "std": 0.025895526649965993,
        "median": 0.8577866554260254,
        "min": 0.8327679634094238,
        "max": 0.9248868227005005,
        "count": 13
      },
      "bert_f1": {
        "mean": 0.8635652569624094,
        "std": 0.024833920880528895,
        "median": 0.8617186546325684,
        "min": 0.8210775852203369,
        "max": 0.914668083190918,
        "count": 13
      },
      "meteor": {
        "mean": 0.04831298597830032,
        "std": 0.06791307573849255,
        "median": 0.0,
        "min": 0.0,
        "max": 0.22222222222222224,
        "count": 13
      },
      "sbert_similarity": {
        "mean": 0.3848857902563535,
        "std": 0.14219413433416397,
        "median": 0.38256001472473145,
        "min": 0.21591627597808838,
        "max": 0.6361753940582275,
        "count": 13
      }
    },
    "category_4": {
      "exact_match": {
        "mean": 0.02857142857142857,
        "std": 0.16780151935092938,
        "median": 0.0,
        "min": 0,
        "max": 1,
        "count": 70
      },
      "f1": {
        "mean": 0.24080260727268926,
        "std": 0.26706720617854557,
        "median": 0.18181818181818182,
        "min": 0.0,
        "max": 1.0,
        "count": 70
      },
      "rouge1_f": {
        "mean": 0.26965902359272015,
        "std": 0.2780302975180893,
        "median": 0.21764705882352942,
        "min": 0.0,
        "max": 1.0,
        "count": 70
      },
      "rouge2_f": {
        "mean": 0.15052589040524675,
        "std": 0.2823585563621282,
        "median": 0.0,
        "min": 0.0,
        "max": 1.0,
        "count": 70
      },
      "rougeL_f": {
        "mean": 0.2544754408524636,
        "std": 0.27754427167813345,
        "median": 0.18070652173913043,
        "min": 0.0,
        "max": 1.0,
        "count": 70
      },
      "bleu1": {
        "mean": 0.19968918014349737,
        "std": 0.2356188614967982,
        "median": 0.14835164835164835,
        "min": 0,
        "max": 1.0,
        "count": 70
      },
      "bleu2": {
        "mean": 0.13959115099013467,
        "std": 0.22693922002104958,
        "median": 0.030860669992418384,
        "min": 0,
        "max": 1.0,
        "count": 70
      },
      "bleu3": {
        "mean": 0.1058414816630349,
        "std": 0.19388341074290227,
        "median": 0.020960689327516144,
        "min": 0,
        "max": 1.0,
        "count": 70
      },
      "bleu4": {
        "mean": 0.0772978967948112,
        "std": 0.14467874712351414,
        "median": 0.01562800152789078,
        "min": 0,
        "max": 0.668740304976422,
        "count": 70
      },
      "bert_precision": {
        "mean": 0.8755537739821843,
        "std": 0.04918435217515289,
        "median": 0.8716830909252167,
        "min": 0.7960872650146484,
        "max": 1.000000238418579,
        "count": 70
      },
      "bert_recall": {
        "mean": 0.885528587443488,
        "std": 0.05169238769991031,
        "median": 0.8798570930957794,
        "min": 0.7678672075271606,
        "max": 1.000000238418579,
        "count": 70
      },
      "bert_f1": {
        "mean": 0.8801740697452001,
        "std": 0.04740189776800163,
        "median": 0.8765144348144531,
        "min": 0.7817226648330688,
        "max": 1.000000238418579,
        "count": 70
      },
      "meteor": {
        "mean": 0.22579972041990615,
        "std": 0.26620902004351943,
        "median": 0.1316701316701317,
        "min": 0.0,
        "max": 0.9814814814814815,
        "count": 70
      },
      "sbert_similarity": {
        "mean": 0.42181828039299163,
        "std": 0.25325647507714516,
        "median": 0.39099574089050293,
        "min": -0.03565456345677376,
        "max": 1.0000001192092896,
        "count": 70
      }
    },
    "category_5": {
      "exact_match": {
        "mean": 0.2978723404255319,
        "std": 0.462267267743469,
        "median": 0,
        "min": 0,
        "max": 1,
        "count": 47
      },
      "f1": {
        "mean": 0.6156618651160742,
        "std": 0.40134705271676935,
        "median": 0.7857142857142858,
        "min": 0.0,
        "max": 1.0,
        "count": 47
      },
      "rouge1_f": {
        "mean": 0.619792146387891,
        "std": 0.39868344615892265,
        "median": 0.761904761904762,
        "min": 0.0,
        "max": 1.0,
        "count": 47
      },
      "rouge2_f": {
        "mean": 0.5067312244533628,
        "std": 0.44024014527961536,
        "median": 0.5263157894736842,
        "min": 0.0,
        "max": 1.0,
        "count": 47
      },
      "rougeL_f": {
        "mean": 0.6132358028102709,
        "std": 0.4032327642952534,
        "median": 0.761904761904762,
        "min": 0.0,
        "max": 1.0,
        "count": 47
      },
      "bleu1": {
        "mean": 0.5376195276800512,
        "std": 0.4050526710913519,
        "median": 0.5,
        "min": 0,
        "max": 1.0,
        "count": 47
      },
      "bleu2": {
        "mean": 0.47717018840223485,
        "std": 0.4058017596193057,
        "median": 0.33968311024337877,
        "min": 0,
        "max": 1.0,
        "count": 47
      },
      "bleu3": {
        "mean": 0.4283340206896642,
        "std": 0.39475420037178877,
        "median": 0.23173297416566233,
        "min": 0,
        "max": 1.0,
        "count": 47
      },
      "bleu4": {
        "mean": 0.3500689305252372,
        "std": 0.35759732275987777,
        "median": 0.1778279410038923,
        "min": 0,
        "max": 1.0,
        "count": 47
      },
      "bert_precision": {
        "mean": 0.9331138666639937,
        "std": 0.06632393666817556,
        "median": 0.9519978761672974,
        "min": 0.807045578956604,
        "max": 1.000000238418579,
        "count": 47
      },
      "bert_recall": {
        "mean": 0.9336735758375614,
        "std": 0.06861405677173622,
        "median": 0.9495986700057983,
        "min": 0.7670419216156006,
        "max": 1.000000238418579,
        "count": 47
      },
      "bert_f1": {
        "mean": 0.9331473264288395,
        "std": 0.06602155121593731,
        "median": 0.9504681825637817,
        "min": 0.7871182560920715,
        "max": 1.000000238418579,
        "count": 47
      },
      "meteor": {
        "mean": 0.5702865327846511,
        "std": 0.3783872147875232,
        "median": 0.6388888888888888,
        "min": 0.0,
        "max": 0.9993141289437586,
        "count": 47
      },
      "sbert_similarity": {
        "mean": 0.6856985793231015,
        "std": 0.3667940062134762,
        "median": 0.8569421768188477,
        "min": -0.004309743642807007,
        "max": 1.0000001192092896,
        "count": 47
      }
    }
  },
  "individual_results": [
    {
      "sample_id": 0,
      "question": "When did Caroline go to the LGBTQ support group?",
      "prediction": "17 July, 2023",
      "reference": "7 May 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.3333333333333333,
        "rouge1_f": 0.3333333333333333,
        "rouge2_f": 0.0,
        "rougeL_f": 0.3333333333333333,
        "bleu1": 0.25,
        "bleu2": 0.09128709291752769,
        "bleu3": 0.07665320007195604,
        "bleu4": 0.08034284189446518,
        "bert_precision": 0.9421913623809814,
        "bert_recall": 0.9628998637199402,
        "bert_f1": 0.9524329900741577,
        "meteor": 0.16666666666666666,
        "sbert_similarity": 0.83634352684021
      }
    },
    {
      "sample_id": 0,
      "question": "When did Melanie paint a sunrise?",
      "prediction": "15 July 2023",
      "reference": 2022,
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.7915204763412476,
        "bert_recall": 0.895067572593689,
        "bert_f1": 0.8401154279708862,
        "meteor": 0.0,
        "sbert_similarity": 0.682317316532135
      }
    },
    {
      "sample_id": 0,
      "question": "What fields would Caroline be likely to pursue in her educaton?",
      "prediction": "education and career options",
      "reference": "Psychology, counseling certification",
      "category": 3,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8631337285041809,
        "bert_recall": 0.8603082299232483,
        "bert_f1": 0.8617186546325684,
        "meteor": 0.0,
        "sbert_similarity": 0.34456634521484375
      }
    },
    {
      "sample_id": 0,
      "question": "What did Caroline research?",
      "prediction": "Caroline researched books to help her discover who she is.",
      "reference": "Adoption agencies",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8456121683120728,
        "bert_recall": 0.834522545337677,
        "bert_f1": 0.8400307297706604,
        "meteor": 0.0,
        "sbert_similarity": 0.19439074397087097
      }
    },
    {
      "sample_id": 0,
      "question": "What is Caroline's identity?",
      "prediction": "Caroline is an LGBTQ+ individual who has come out in her personal journey.",
      "reference": "Transgender woman",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.807786762714386,
        "bert_recall": 0.853186845779419,
        "bert_f1": 0.8298664093017578,
        "meteor": 0.0,
        "sbert_similarity": 0.4287770390510559
      }
    },
    {
      "sample_id": 0,
      "question": "When did Melanie run a charity race?",
      "prediction": "1:14 pm on 25 May, 2023",
      "reference": "The sunday before 25 May 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.5,
        "rouge1_f": 0.4615384615384615,
        "rouge2_f": 0.3636363636363636,
        "rougeL_f": 0.4615384615384615,
        "bleu1": 0.42857142857142855,
        "bleu2": 0.26726124191242434,
        "bleu3": 0.11511121735118796,
        "bleu4": 0.07730551756939454,
        "bert_precision": 0.8710809350013733,
        "bert_recall": 0.8605568408966064,
        "bert_f1": 0.8657869100570679,
        "meteor": 0.16666666666666666,
        "sbert_similarity": 0.8002878427505493
      }
    },
    {
      "sample_id": 0,
      "question": "When is Melanie planning on going camping?",
      "prediction": "On 27 June, 2023.",
      "reference": "June 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.6666666666666666,
        "rouge1_f": 0.6666666666666666,
        "rouge2_f": 0.5,
        "rougeL_f": 0.6666666666666666,
        "bleu1": 0.3333333333333333,
        "bleu2": 0.08164965809277261,
        "bleu3": 0.05665132701906204,
        "bleu4": 0.048549177170732344,
        "bert_precision": 0.8836289644241333,
        "bert_recall": 0.9467310905456543,
        "bert_f1": 0.9140923023223877,
        "meteor": 0.0,
        "sbert_similarity": 0.8768488168716431
      }
    },
    {
      "sample_id": 0,
      "question": "What is Caroline's relationship status?",
      "prediction": "Caroline and Mel are currently in a supportive relationship.",
      "reference": "Single",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8316842317581177,
        "bert_recall": 0.8633854985237122,
        "bert_f1": 0.8472384214401245,
        "meteor": 0.0,
        "sbert_similarity": 0.1847463846206665
      }
    },
    {
      "sample_id": 0,
      "question": "When did Caroline give a speech at a school?",
      "prediction": "8 July 2023",
      "reference": "The week before 9 June 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.2222222222222222,
        "rouge1_f": 0.2222222222222222,
        "rouge2_f": 0.0,
        "rougeL_f": 0.2222222222222222,
        "bleu1": 0.12262648039048077,
        "bleu2": 0.04749303163563625,
        "bleu3": 0.044556965439969995,
        "bleu4": 0.0417991745584234,
        "bert_precision": 0.9431148767471313,
        "bert_recall": 0.8701103925704956,
        "bert_f1": 0.9051429629325867,
        "meteor": 0.08771929824561403,
        "sbert_similarity": 0.7566318511962891
      }
    },
    {
      "sample_id": 0,
      "question": "When did Caroline meet up with her friends, family, and mentors?",
      "prediction": "7 June 2023",
      "reference": "The week before 9 June 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.4444444444444444,
        "rouge1_f": 0.4444444444444444,
        "rouge2_f": 0.28571428571428575,
        "rougeL_f": 0.4444444444444444,
        "bleu1": 0.24525296078096154,
        "bleu2": 0.21239529438966132,
        "bleu3": 0.11974448045963429,
        "bleu4": 0.08839449202722564,
        "bert_precision": 0.953313946723938,
        "bert_recall": 0.879320502281189,
        "bert_f1": 0.9148234724998474,
        "meteor": 0.3289473684210526,
        "sbert_similarity": 0.7972956895828247
      }
    },
    {
      "sample_id": 0,
      "question": "How long has Caroline had her current group of friends for?",
      "prediction": "1 year and 5 months",
      "reference": "4 years",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.28571428571428575,
        "rouge2_f": 0.0,
        "rougeL_f": 0.28571428571428575,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.7988443374633789,
        "bert_recall": 0.918238639831543,
        "bert_f1": 0.854390561580658,
        "meteor": 0.21739130434782608,
        "sbert_similarity": 0.6317654848098755
      }
    },
    {
      "sample_id": 0,
      "question": "Where did Caroline move from 4 years ago?",
      "prediction": "No place known",
      "reference": "Sweden",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8189002275466919,
        "bert_recall": 0.8747500777244568,
        "bert_f1": 0.8459043502807617,
        "meteor": 0.0,
        "sbert_similarity": 0.28696393966674805
      }
    },
    {
      "sample_id": 0,
      "question": "How long ago was Caroline's 18th birthday?",
      "prediction": "2023-8-17",
      "reference": "10 years ago",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.7294246554374695,
        "bert_recall": 0.7505234479904175,
        "bert_f1": 0.7398236989974976,
        "meteor": 0.0,
        "sbert_similarity": 0.2211436629295349
      }
    },
    {
      "sample_id": 0,
      "question": "What career path has Caroline decided to persue?",
      "prediction": "counseling and mental health career",
      "reference": "counseling or mental health for Transgender people",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.5,
        "rouge1_f": 0.5,
        "rouge2_f": 0.2,
        "rougeL_f": 0.5,
        "bleu1": 0.40219202762138356,
        "bleu2": 0.259613837492502,
        "bleu3": 0.11666546293023573,
        "bleu4": 0.08428828344718171,
        "bert_precision": 0.903893232345581,
        "bert_recall": 0.8744668960571289,
        "bert_f1": 0.8889365792274475,
        "meteor": 0.3758169934640523,
        "sbert_similarity": 0.5973507761955261
      }
    },
    {
      "sample_id": 0,
      "question": "Would Caroline still want to pursue counseling as a career if she hadn't received support growing up?",
      "prediction": "Yes, if she had the support from her own experiences.",
      "reference": "Likely no",
      "category": 3,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8199819922447205,
        "bert_recall": 0.8327679634094238,
        "bert_f1": 0.8263254761695862,
        "meteor": 0.0,
        "sbert_similarity": 0.22823210060596466
      }
    },
    {
      "sample_id": 0,
      "question": "What activities does Melanie partake in?",
      "prediction": "creative expression and relaxation",
      "reference": "pottery, camping, painting, swimming",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8525632619857788,
        "bert_recall": 0.8404351472854614,
        "bert_f1": 0.8464557528495789,
        "meteor": 0.0,
        "sbert_similarity": 0.3759300112724304
      }
    },
    {
      "sample_id": 0,
      "question": "When did Melanie sign up for a pottery class?",
      "prediction": "July 3, 2023",
      "reference": "2 July 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.6666666666666666,
        "rouge1_f": 0.6666666666666666,
        "rouge2_f": 0.0,
        "rougeL_f": 0.6666666666666666,
        "bleu1": 0.5,
        "bleu2": 0.12909944487358058,
        "bleu3": 0.09635409769034631,
        "bleu4": 0.09554427922043669,
        "bert_precision": 0.9193636178970337,
        "bert_recall": 0.9308592081069946,
        "bert_f1": 0.9250756502151489,
        "meteor": 0.3333333333333333,
        "sbert_similarity": 0.9382219314575195
      }
    },
    {
      "sample_id": 0,
      "question": "When is Caroline going to the transgender conference?",
      "prediction": "1:36 pm on 3 July, 2023",
      "reference": "July 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.5,
        "rouge1_f": 0.4444444444444445,
        "rouge2_f": 0.2857142857142857,
        "rougeL_f": 0.4444444444444445,
        "bleu1": 0.2857142857142857,
        "bleu2": 0.06900655593423542,
        "bleu3": 0.047098549390932394,
        "bleu4": 0.039281465090051315,
        "bert_precision": 0.7941243648529053,
        "bert_recall": 0.8709076046943665,
        "bert_f1": 0.8307455778121948,
        "meteor": 0.20833333333333334,
        "sbert_similarity": 0.7499395608901978
      }
    },
    {
      "sample_id": 0,
      "question": "Where has Melanie camped?",
      "prediction": "Melanie has camped at a place in the mountains.",
      "reference": "beach, mountains, forest",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.16666666666666666,
        "rouge1_f": 0.16666666666666666,
        "rouge2_f": 0.0,
        "rougeL_f": 0.16666666666666666,
        "bleu1": 0.10000000000000002,
        "bleu2": 0.03333333333333335,
        "bleu3": 0.024950558772614845,
        "bleu4": 0.021105340631872645,
        "bert_precision": 0.8587822914123535,
        "bert_recall": 0.8392829895019531,
        "bert_f1": 0.8489206433296204,
        "meteor": 0.0,
        "sbert_similarity": 0.2893139123916626
      }
    },
    {
      "sample_id": 0,
      "question": "What do Melanie's kids like?",
      "prediction": "Melanie's kids love painting and nature-related activities.",
      "reference": "dinosaurs, nature",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.1818181818181818,
        "rouge2_f": 0.0,
        "rougeL_f": 0.1818181818181818,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8563957214355469,
        "bert_recall": 0.8504297733306885,
        "bert_f1": 0.8534023761749268,
        "meteor": 0.0,
        "sbert_similarity": 0.24236831068992615
      }
    },
    {
      "sample_id": 0,
      "question": "When did Melanie go to the museum?",
      "prediction": "On 6 July, 2023.",
      "reference": "5 July 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.5714285714285715,
        "rouge1_f": 0.5714285714285715,
        "rouge2_f": 0.4,
        "rougeL_f": 0.5714285714285715,
        "bleu1": 0.3333333333333333,
        "bleu2": 0.08164965809277261,
        "bleu3": 0.05665132701906204,
        "bleu4": 0.048549177170732344,
        "bert_precision": 0.9385285377502441,
        "bert_recall": 0.9713683128356934,
        "bert_f1": 0.9546660780906677,
        "meteor": 0.0,
        "sbert_similarity": 0.8712024688720703
      }
    },
    {
      "sample_id": 0,
      "question": "When did Caroline have a picnic?",
      "prediction": "8:56 pm on 20 July, 2023",
      "reference": "The week before 6 July 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.3333333333333333,
        "rouge1_f": 0.30769230769230765,
        "rouge2_f": 0.1818181818181818,
        "rougeL_f": 0.30769230769230765,
        "bleu1": 0.2857142857142857,
        "bleu2": 0.06900655593423542,
        "bleu3": 0.047098549390932394,
        "bleu4": 0.039281465090051315,
        "bert_precision": 0.8689742684364319,
        "bert_recall": 0.8774502277374268,
        "bert_f1": 0.8731917142868042,
        "meteor": 0.08333333333333333,
        "sbert_similarity": 0.6537618637084961
      }
    },
    {
      "sample_id": 0,
      "question": "Would Caroline likely have Dr. Seuss books on her bookshelf?",
      "prediction": "No, Caroline would not likely have Dr. Seuss books on her bookshelf.",
      "reference": "Yes, since she collects classic children's books",
      "category": 3,
      "metrics": {
        "exact_match": 0,
        "f1": 0.10526315789473685,
        "rouge1_f": 0.1,
        "rouge2_f": 0.0,
        "rougeL_f": 0.1,
        "bleu1": 0.14285714285714285,
        "bleu2": 0.033149677206589796,
        "bleu3": 0.021746357799665227,
        "bleu4": 0.0169861974906263,
        "bert_precision": 0.8814523816108704,
        "bert_recall": 0.897178053855896,
        "bert_f1": 0.8892456293106079,
        "meteor": 0.06666666666666667,
        "sbert_similarity": 0.541889488697052
      }
    },
    {
      "sample_id": 0,
      "question": "What books has Melanie read?",
      "prediction": "Books: [book1, book2, ...]",
      "reference": "\"Nothing is Impossible\", \"Charlotte's Web\"",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0.08897082254631197,
        "bleu2": 0.02984172108414368,
        "bleu3": 0.022474396477098004,
        "bleu4": 0.019201920550239778,
        "bert_precision": 0.8077356815338135,
        "bert_recall": 0.8140280246734619,
        "bert_f1": 0.8108696341514587,
        "meteor": 0.0,
        "sbert_similarity": 0.10085391253232956
      }
    },
    {
      "sample_id": 0,
      "question": "What does Melanie do to destress?",
      "prediction": "Melanie uses art and pottery to express her emotions and find joy, which helps her destress.",
      "reference": "Running, pottery",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.125,
        "rouge1_f": 0.1111111111111111,
        "rouge2_f": 0.0,
        "rougeL_f": 0.1111111111111111,
        "bleu1": 0.11111111111111109,
        "bleu2": 0.02556549962824567,
        "bleu3": 0.016660661223010558,
        "bleu4": 0.01284618972676772,
        "bert_precision": 0.8434330224990845,
        "bert_recall": 0.8890897631645203,
        "bert_f1": 0.8656598329544067,
        "meteor": 0.1470588235294118,
        "sbert_similarity": 0.40072715282440186
      }
    },
    {
      "sample_id": 0,
      "question": "When did Caroline go to the LGBTQ conference?",
      "prediction": "2023-07-12",
      "reference": "10 July 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.3333333333333333,
        "rouge2_f": 0.0,
        "rougeL_f": 0.3333333333333333,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.7812943458557129,
        "bert_recall": 0.8105916976928711,
        "bert_f1": 0.7956734299659729,
        "meteor": 0.0,
        "sbert_similarity": 0.6710850596427917
      }
    },
    {
      "sample_id": 0,
      "question": "When did Melanie read the book \"nothing is impossible\"?",
      "prediction": "2023-12-12",
      "reference": 2022,
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.7127465605735779,
        "bert_recall": 0.8817129135131836,
        "bert_f1": 0.7882769107818604,
        "meteor": 0.0,
        "sbert_similarity": 0.7095960378646851
      }
    },
    {
      "sample_id": 0,
      "question": "Would Caroline pursue writing as a career option?",
      "prediction": "Yes, Caroline has been exploring her career options by continuing education and checking out new fields like counseling and mental health.",
      "reference": "LIkely no; though she likes reading, she wants to be a counselor",
      "category": 3,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.060606060606060615,
        "rouge2_f": 0.0,
        "rougeL_f": 0.060606060606060615,
        "bleu1": 0.043478260869565216,
        "bleu2": 0.014058038927888336,
        "bleu3": 0.010263558503321437,
        "bleu4": 0.008282282660969604,
        "bert_precision": 0.8587861657142639,
        "bert_recall": 0.8543213605880737,
        "bert_f1": 0.8565478920936584,
        "meteor": 0.11627906976744187,
        "sbert_similarity": 0.4755170941352844
      }
    },
    {
      "sample_id": 0,
      "question": "When did Caroline go to the adoption meeting?",
      "prediction": "15 July, 2023",
      "reference": "The friday before 15 July 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.6666666666666666,
        "rouge1_f": 0.6666666666666666,
        "rouge2_f": 0.5714285714285715,
        "rougeL_f": 0.6666666666666666,
        "bleu1": 0.45489799478447507,
        "bleu2": 0.30326532985631666,
        "bleu3": 0.14283448174223617,
        "bleu4": 0.11404605374835301,
        "bert_precision": 0.9278002977371216,
        "bert_recall": 0.8661315441131592,
        "bert_f1": 0.8959059119224548,
        "meteor": 0.17543859649122806,
        "sbert_similarity": 0.7955405712127686
      }
    },
    {
      "sample_id": 0,
      "question": "When did Melanie go to the pottery workshop?",
      "prediction": "12:09 am on 13 September, 2023",
      "reference": "The Friday before 15 July 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.16666666666666666,
        "rouge1_f": 0.15384615384615383,
        "rouge2_f": 0.0,
        "rougeL_f": 0.15384615384615383,
        "bleu1": 0.14285714285714285,
        "bleu2": 0.048795003647426664,
        "bleu3": 0.037468614372419724,
        "bleu4": 0.033031643180138064,
        "bert_precision": 0.8616957068443298,
        "bert_recall": 0.8685836791992188,
        "bert_f1": 0.8651259541511536,
        "meteor": 0.08333333333333333,
        "sbert_similarity": 0.7093815803527832
      }
    },
    {
      "sample_id": 0,
      "question": "Would Melanie be considered a member of the LGBTQ community?",
      "prediction": "Yes, Melanie is a member of the LGBTQ community.",
      "reference": "Likely no, she does not refer to herself as part of it",
      "category": 3,
      "metrics": {
        "exact_match": 0,
        "f1": 0.09523809523809525,
        "rouge1_f": 0.09523809523809525,
        "rouge2_f": 0.0,
        "rougeL_f": 0.09523809523809525,
        "bleu1": 0.15159143965003283,
        "bleu2": 0.03555134387944182,
        "bleu3": 0.0235406066879502,
        "bleu4": 0.018690183721524995,
        "bert_precision": 0.8803388476371765,
        "bert_recall": 0.8577866554260254,
        "bert_f1": 0.8689164519309998,
        "meteor": 0.042735042735042736,
        "sbert_similarity": 0.5069262385368347
      }
    },
    {
      "sample_id": 0,
      "question": "When did Melanie go camping in June?",
      "prediction": "10:37 am on 27 June, 2023",
      "reference": "The week before 27 June 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.5,
        "rouge1_f": 0.4615384615384615,
        "rouge2_f": 0.3636363636363636,
        "rougeL_f": 0.4615384615384615,
        "bleu1": 0.42857142857142855,
        "bleu2": 0.26726124191242434,
        "bleu3": 0.11511121735118796,
        "bleu4": 0.07730551756939454,
        "bert_precision": 0.8706039190292358,
        "bert_recall": 0.8794524073600769,
        "bert_f1": 0.8750057816505432,
        "meteor": 0.16666666666666666,
        "sbert_similarity": 0.6847915649414062
      }
    },
    {
      "sample_id": 0,
      "question": "What LGBTQ+ events has Caroline participated in?",
      "prediction": "Caroline organized an LGBTQ art show next month, participated in a transgender conference this month, and had a support group with LGBTQ+ community members.",
      "reference": "Pride parade, school speech, support group",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.14285714285714288,
        "rouge1_f": 0.13333333333333333,
        "rouge2_f": 0.07142857142857142,
        "rougeL_f": 0.13333333333333333,
        "bleu1": 0.14814814814814814,
        "bleu2": 0.0754851356096397,
        "bleu3": 0.02938112002170205,
        "bleu4": 0.017554679594930896,
        "bert_precision": 0.8531489372253418,
        "bert_recall": 0.8749974966049194,
        "bert_f1": 0.863935112953186,
        "meteor": 0.24038461538461542,
        "sbert_similarity": 0.3606012463569641
      }
    },
    {
      "sample_id": 0,
      "question": "When did Caroline go to a pride parade during the summer?",
      "prediction": "20 July 2023",
      "reference": "The week before 3 July 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.4444444444444444,
        "rouge1_f": 0.4444444444444444,
        "rouge2_f": 0.28571428571428575,
        "rougeL_f": 0.4444444444444444,
        "bleu1": 0.24525296078096154,
        "bleu2": 0.21239529438966132,
        "bleu3": 0.11974448045963429,
        "bleu4": 0.08839449202722564,
        "bert_precision": 0.9524893760681152,
        "bert_recall": 0.8824767470359802,
        "bert_f1": 0.9161473512649536,
        "meteor": 0.3289473684210526,
        "sbert_similarity": 0.7179256677627563
      }
    },
    {
      "sample_id": 0,
      "question": "What events has Caroline participated in to help children?",
      "prediction": "Caroline has participated in events like volunteering at an LGBTQ+ youth center and organizing a school event for kids, as well as joining a mentorship program for LGBTQ youth.",
      "reference": "Mentoring program, school speech",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.13793103448275865,
        "rouge1_f": 0.1212121212121212,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0606060606060606,
        "bleu1": 0.09677419354838708,
        "bleu2": 0.017960530202677495,
        "bleu3": 0.010845742813191119,
        "bleu4": 0.007939087147543497,
        "bert_precision": 0.8335275650024414,
        "bert_recall": 0.8568161725997925,
        "bert_f1": 0.8450114130973816,
        "meteor": 0.07692307692307693,
        "sbert_similarity": 0.38119542598724365
      }
    },
    {
      "sample_id": 0,
      "question": "When did Melanie go camping in July?",
      "prediction": "15 July, 2023",
      "reference": "two weekends before 17 July 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.4444444444444444,
        "rouge1_f": 0.4444444444444444,
        "rouge2_f": 0.28571428571428575,
        "rougeL_f": 0.4444444444444444,
        "bleu1": 0.3032653298563167,
        "bleu2": 0.07830277146770757,
        "bleu3": 0.05844171443814127,
        "bleu4": 0.05795053470733952,
        "bert_precision": 0.9177718758583069,
        "bert_recall": 0.8805042505264282,
        "bert_f1": 0.8987518548965454,
        "meteor": 0.08771929824561403,
        "sbert_similarity": 0.645390510559082
      }
    },
    {
      "sample_id": 0,
      "question": "When did Caroline join a mentorship program?",
      "prediction": "On 20 October, 2023.",
      "reference": "The weekend before 17 July 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.2,
        "rouge1_f": 0.2,
        "rouge2_f": 0.0,
        "rougeL_f": 0.2,
        "bleu1": 0.16666666666666669,
        "bleu2": 0.05773502691896258,
        "bleu3": 0.04506819749679463,
        "bleu4": 0.040824829046386304,
        "bert_precision": 0.9259936213493347,
        "bert_recall": 0.8822894096374512,
        "bert_f1": 0.9036133289337158,
        "meteor": 0.0,
        "sbert_similarity": 0.724979043006897
      }
    },
    {
      "sample_id": 0,
      "question": "What did Melanie paint recently?",
      "prediction": "A group of bowls and a starfish on a white surface in the image.",
      "reference": "sunset",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.7935880422592163,
        "bert_recall": 0.8286675810813904,
        "bert_f1": 0.8107485175132751,
        "meteor": 0.0,
        "sbert_similarity": 0.1639835089445114
      }
    },
    {
      "sample_id": 0,
      "question": "What activities has Melanie done with her family?",
      "prediction": "Melanie has done activities such as camping with her family and creating art in the style of pottery.",
      "reference": "Pottery, painting, camping, museum, swimming, hiking",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.16666666666666666,
        "rouge1_f": 0.16666666666666666,
        "rouge2_f": 0.0,
        "rougeL_f": 0.08333333333333333,
        "bleu1": 0.10526315789473682,
        "bleu2": 0.024182541670333724,
        "bleu3": 0.015742117131956883,
        "bleu4": 0.012109013026441871,
        "bert_precision": 0.8562169075012207,
        "bert_recall": 0.8486282229423523,
        "bert_f1": 0.8524056673049927,
        "meteor": 0.0,
        "sbert_similarity": 0.46982043981552124
      }
    },
    {
      "sample_id": 0,
      "question": "In what ways is Caroline participating in the LGBTQ community?",
      "prediction": "Caroline is participating in the LGBTQ community by supporting LGBTQ+ activists, organizing events and campaigns, and spreading awareness through her work in the field.",
      "reference": "Joining activist group, going to pride parades, participating in an art show, mentoring program",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.11428571428571427,
        "rouge1_f": 0.15789473684210525,
        "rouge2_f": 0.05555555555555555,
        "rougeL_f": 0.10526315789473685,
        "bleu1": 0.14814814814814814,
        "bleu2": 0.0754851356096397,
        "bleu3": 0.02938112002170205,
        "bleu4": 0.017554679594930896,
        "bert_precision": 0.8513879776000977,
        "bert_recall": 0.8429065346717834,
        "bert_f1": 0.8471260666847229,
        "meteor": 0.09999999999999999,
        "sbert_similarity": 0.4594075083732605
      }
    },
    {
      "sample_id": 0,
      "question": "How many times has Melanie gone to the beach in 2023?",
      "prediction": "1 time",
      "reference": 2,
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8732349276542664,
        "bert_recall": 0.9215748906135559,
        "bert_f1": 0.8967539668083191,
        "meteor": 0.0,
        "sbert_similarity": 0.4973270297050476
      }
    },
    {
      "sample_id": 0,
      "question": "When did Caroline join a new activist group?",
      "prediction": "2023-10-12",
      "reference": "The Tuesday before 20 July 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.2222222222222222,
        "rouge2_f": 0.0,
        "rougeL_f": 0.2222222222222222,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.7718344330787659,
        "bert_recall": 0.7782840728759766,
        "bert_f1": 0.7750458717346191,
        "meteor": 0.0,
        "sbert_similarity": 0.5826021432876587
      }
    },
    {
      "sample_id": 0,
      "question": "Would Melanie be more interested in going to a national park or a theme park?",
      "prediction": "Melanie would prefer a theme park over a national park.",
      "reference": "National park; she likes the outdoors",
      "category": 3,
      "metrics": {
        "exact_match": 0,
        "f1": 0.14285714285714288,
        "rouge1_f": 0.25,
        "rouge2_f": 0.14285714285714285,
        "rougeL_f": 0.25,
        "bleu1": 0.18181818181818182,
        "bleu2": 0.13483997249264845,
        "bleu3": 0.06036431606009924,
        "bleu4": 0.03986357128268015,
        "bert_precision": 0.8717861175537109,
        "bert_recall": 0.8820949792861938,
        "bert_f1": 0.8769102096557617,
        "meteor": 0.078125,
        "sbert_similarity": 0.6361753940582275
      }
    },
    {
      "sample_id": 0,
      "question": "What kind of art does Caroline make?",
      "prediction": "art and self-expression",
      "reference": "abstract art",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.4,
        "rouge1_f": 0.3333333333333333,
        "rouge2_f": 0.0,
        "rougeL_f": 0.3333333333333333,
        "bleu1": 0.3333333333333333,
        "bleu2": 0.12909944487358058,
        "bleu3": 0.12111838948674812,
        "bleu4": 0.11362193664674995,
        "bert_precision": 0.8654249906539917,
        "bert_recall": 0.8673126697540283,
        "bert_f1": 0.8663678169250488,
        "meteor": 0.2380952380952381,
        "sbert_similarity": 0.582911491394043
      }
    },
    {
      "sample_id": 0,
      "question": "When is Melanie's daughter's birthday?",
      "prediction": "8:56 pm on 20 July, 2023",
      "reference": "13 August",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.7943932414054871,
        "bert_recall": 0.8350908160209656,
        "bert_f1": 0.8142338395118713,
        "meteor": 0.0,
        "sbert_similarity": 0.46568000316619873
      }
    },
    {
      "sample_id": 0,
      "question": "When did Caroline attend a pride parade in August?",
      "prediction": "8:56 pm on 17 August, 2023",
      "reference": "The Friday before 14 August 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.3333333333333333,
        "rouge1_f": 0.30769230769230765,
        "rouge2_f": 0.1818181818181818,
        "rougeL_f": 0.30769230769230765,
        "bleu1": 0.2857142857142857,
        "bleu2": 0.06900655593423542,
        "bleu3": 0.047098549390932394,
        "bleu4": 0.039281465090051315,
        "bert_precision": 0.8719480037689209,
        "bert_recall": 0.8765730857849121,
        "bert_f1": 0.8742544054985046,
        "meteor": 0.08333333333333333,
        "sbert_similarity": 0.7429739236831665
      }
    },
    {
      "sample_id": 0,
      "question": "Would Melanie be considered an ally to the transgender community?",
      "prediction": "Yes, Melanie is an ally to the transgender community.",
      "reference": "Yes, she is supportive",
      "category": 3,
      "metrics": {
        "exact_match": 0,
        "f1": 0.30769230769230765,
        "rouge1_f": 0.30769230769230765,
        "rouge2_f": 0.0,
        "rougeL_f": 0.30769230769230765,
        "bleu1": 0.2727272727272727,
        "bleu2": 0.1651445647689541,
        "bleu3": 0.06900656343516053,
        "bleu4": 0.04411629359322707,
        "bert_precision": 0.9046727418899536,
        "bert_recall": 0.9248868227005005,
        "bert_f1": 0.914668083190918,
        "meteor": 0.22222222222222224,
        "sbert_similarity": 0.4398810863494873
      }
    },
    {
      "sample_id": 0,
      "question": "Who supports Caroline when she has a negative experience?",
      "prediction": "Melanie and Carolines",
      "reference": "Her mentors, family, and friends",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.25,
        "rouge1_f": 0.25,
        "rouge2_f": 0.0,
        "rougeL_f": 0.25,
        "bleu1": 0.08786571270524224,
        "bleu2": 0.03403024420100487,
        "bleu3": 0.031926460841892726,
        "bleu4": 0.029950417327249696,
        "bert_precision": 0.8484957218170166,
        "bert_recall": 0.8593975305557251,
        "bert_f1": 0.853911817073822,
        "meteor": 0.10416666666666666,
        "sbert_similarity": 0.31333622336387634
      }
    },
    {
      "sample_id": 0,
      "question": "What types of pottery have Melanie and her kids made?",
      "prediction": "pottery, clay sculptures, and bowls",
      "reference": "bowls, cup",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.28571428571428575,
        "rouge1_f": 0.28571428571428575,
        "rouge2_f": 0.0,
        "rougeL_f": 0.28571428571428575,
        "bleu1": 0.2857142857142857,
        "bleu2": 0.06900655593423542,
        "bleu3": 0.047098549390932394,
        "bleu4": 0.039281465090051315,
        "bert_precision": 0.8358965516090393,
        "bert_recall": 0.8803116679191589,
        "bert_f1": 0.8575294017791748,
        "meteor": 0.0,
        "sbert_similarity": 0.48861128091812134
      }
    },
    {
      "sample_id": 0,
      "question": "When did Caroline and Melanie go to a pride fesetival together?",
      "prediction": "October 12, 2023",
      "reference": 2022,
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.7658039927482605,
        "bert_recall": 0.8611037731170654,
        "bert_f1": 0.8106626272201538,
        "meteor": 0.0,
        "sbert_similarity": 0.6342955827713013
      }
    },
    {
      "sample_id": 0,
      "question": "What would Caroline's political leaning likely be?",
      "prediction": "Caroline is a supporter of LGBTQ+ rights and promotes inclusivity, which aligns with her values.",
      "reference": "Liberal",
      "category": 3,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.7920843362808228,
        "bert_recall": 0.8522738814353943,
        "bert_f1": 0.8210775852203369,
        "meteor": 0.0,
        "sbert_similarity": 0.24232132732868195
      }
    },
    {
      "sample_id": 0,
      "question": "What has Melanie painted?",
      "prediction": "Melanie painted a sunset with pink sky and colors inspired by the sunsets.",
      "reference": "Horse, sunset, sunrise",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.125,
        "rouge1_f": 0.125,
        "rouge2_f": 0.0,
        "rougeL_f": 0.125,
        "bleu1": 0.07142857142857141,
        "bleu2": 0.023440361546924773,
        "bleu3": 0.017300021018422027,
        "bleu4": 0.01428363257865929,
        "bert_precision": 0.8374605774879456,
        "bert_recall": 0.8583094477653503,
        "bert_f1": 0.8477568030357361,
        "meteor": 0.0,
        "sbert_similarity": 0.4706650972366333
      }
    },
    {
      "sample_id": 0,
      "question": "What are Melanie's pets' names?",
      "prediction": "Oscar and Bailey",
      "reference": "Oliver, Luna, Bailey",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.3333333333333333,
        "rouge1_f": 0.3333333333333333,
        "rouge2_f": 0.0,
        "rougeL_f": 0.3333333333333333,
        "bleu1": 0.17113903967753066,
        "bleu2": 0.06628186505570068,
        "bleu3": 0.0621842545921536,
        "bleu4": 0.05833544737207805,
        "bert_precision": 0.9022786021232605,
        "bert_recall": 0.8600674867630005,
        "bert_f1": 0.880667507648468,
        "meteor": 0.16666666666666666,
        "sbert_similarity": 0.5424371361732483
      }
    },
    {
      "sample_id": 0,
      "question": "When did Caroline apply to adoption agencies?",
      "prediction": "On 23 August, 2023.",
      "reference": "The week of 23 August 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.6,
        "rouge1_f": 0.6,
        "rouge2_f": 0.5,
        "rougeL_f": 0.6,
        "bleu1": 0.5,
        "bleu2": 0.316227766016838,
        "bleu3": 0.1384586850774637,
        "bleu4": 0.09554427922043669,
        "bert_precision": 0.9394856691360474,
        "bert_recall": 0.8933118581771851,
        "bert_f1": 0.9158170819282532,
        "meteor": 0.08620689655172413,
        "sbert_similarity": 0.8219829797744751
      }
    },
    {
      "sample_id": 0,
      "question": "When did Caroline draw a self-portrait?",
      "prediction": "On 14 July, 2023.",
      "reference": "The week before 23 August 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.2,
        "rouge1_f": 0.2,
        "rouge2_f": 0.0,
        "rougeL_f": 0.2,
        "bleu1": 0.16666666666666669,
        "bleu2": 0.05773502691896258,
        "bleu3": 0.04506819749679463,
        "bleu4": 0.040824829046386304,
        "bert_precision": 0.9285914897918701,
        "bert_recall": 0.8859047889709473,
        "bert_f1": 0.9067460298538208,
        "meteor": 0.0,
        "sbert_similarity": 0.7360950112342834
      }
    },
    {
      "sample_id": 0,
      "question": "What subject have Caroline and Melanie both painted?",
      "prediction": "art",
      "reference": "Sunsets",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8698028922080994,
        "bert_recall": 0.8025693893432617,
        "bert_f1": 0.8348346948623657,
        "meteor": 0.0,
        "sbert_similarity": 0.2876323163509369
      }
    },
    {
      "sample_id": 0,
      "question": "What symbols are important to Caroline?",
      "prediction": "carriage and heart, necklace, cross, flower symbolism, family, love, support, acceptance, inclusivity, community, and identity",
      "reference": "Rainbow flag, transgender symbol",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.10526315789473685,
        "rouge2_f": 0.0,
        "rougeL_f": 0.10526315789473685,
        "bleu1": 0.04000000000000001,
        "bleu2": 0.012909944487358061,
        "bleu3": 0.009415424419114833,
        "bleu4": 0.007575731225158965,
        "bert_precision": 0.8063243627548218,
        "bert_recall": 0.8500356674194336,
        "bert_f1": 0.827603280544281,
        "meteor": 0.0,
        "sbert_similarity": 0.3650718927383423
      }
    },
    {
      "sample_id": 0,
      "question": "When did Caroline encounter people on a hike and have a negative experience?",
      "prediction": "On 3 July, 2023, Caroline encountered people on a hike and had a negative experience.",
      "reference": "The week before 25 August 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.10526315789473684,
        "rouge1_f": 0.09523809523809522,
        "rouge2_f": 0.0,
        "rougeL_f": 0.09523809523809522,
        "bleu1": 0.05555555555555554,
        "bleu2": 0.018077538151554672,
        "bleu3": 0.013254163846385744,
        "bleu4": 0.010802314890908065,
        "bert_precision": 0.8650022745132446,
        "bert_recall": 0.8775244355201721,
        "bert_f1": 0.8712183237075806,
        "meteor": 0.0,
        "sbert_similarity": 0.3284309208393097
      }
    },
    {
      "sample_id": 0,
      "question": "When did Melanie make a plate in pottery class?",
      "prediction": "12:09 am on 13 September, 2023",
      "reference": "24 August 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.2222222222222222,
        "rouge1_f": 0.2,
        "rouge2_f": 0.0,
        "rougeL_f": 0.2,
        "bleu1": 0.14285714285714285,
        "bleu2": 0.048795003647426664,
        "bleu3": 0.037468614372419724,
        "bleu4": 0.033031643180138064,
        "bert_precision": 0.8435789346694946,
        "bert_recall": 0.9102281332015991,
        "bert_f1": 0.8756371140480042,
        "meteor": 0.15151515151515152,
        "sbert_similarity": 0.7296331524848938
      }
    },
    {
      "sample_id": 0,
      "question": "Would Caroline be considered religious?",
      "prediction": "Caroline is not religious.",
      "reference": "Somewhat, but not extremely religious",
      "category": 3,
      "metrics": {
        "exact_match": 0,
        "f1": 0.4444444444444445,
        "rouge1_f": 0.4444444444444445,
        "rouge2_f": 0.0,
        "rougeL_f": 0.4444444444444445,
        "bleu1": 0.3274923012311928,
        "bleu2": 0.08187307530779821,
        "bleu3": 0.05830302516643934,
        "bleu4": 0.05231223689135342,
        "bert_precision": 0.9097069501876831,
        "bert_recall": 0.8578667640686035,
        "bert_f1": 0.883026659488678,
        "meteor": 0.10204081632653061,
        "sbert_similarity": 0.5011173486709595
      }
    },
    {
      "sample_id": 0,
      "question": "What instruments does Melanie play?",
      "prediction": "guitar and piano",
      "reference": "clarinet and violin",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.3333333333333333,
        "rouge1_f": 0.3333333333333333,
        "rouge2_f": 0.0,
        "rougeL_f": 0.3333333333333333,
        "bleu1": 0.3333333333333333,
        "bleu2": 0.12909944487358058,
        "bleu3": 0.12111838948674812,
        "bleu4": 0.11362193664674995,
        "bert_precision": 0.8956488370895386,
        "bert_recall": 0.8660413026809692,
        "bert_f1": 0.8805962204933167,
        "meteor": 0.16666666666666666,
        "sbert_similarity": 0.5271011590957642
      }
    },
    {
      "sample_id": 0,
      "question": "What musical artists/bands has Melanie seen?",
      "prediction": "Melanie is from the band 'Lover Songs' and has performed at a concert called 'Summer Sounds'. This is one of the musical artists/bands she has seen.",
      "reference": "Summer Sounds, Matt Patterson",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.12903225806451613,
        "rouge2_f": 0.06896551724137931,
        "rougeL_f": 0.12903225806451613,
        "bleu1": 0.03333333333333333,
        "bleu2": 0.010721125348377946,
        "bleu3": 0.007805423943497505,
        "bleu4": 0.006244385951234721,
        "bert_precision": 0.8264094591140747,
        "bert_recall": 0.8650514483451843,
        "bert_f1": 0.8452891111373901,
        "meteor": 0.0,
        "sbert_similarity": 0.3644786477088928
      }
    },
    {
      "sample_id": 0,
      "question": "When did Melanie go to the park?",
      "prediction": "August 17, 2023",
      "reference": "27 August 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.6666666666666666,
        "rouge1_f": 0.6666666666666666,
        "rouge2_f": 0.0,
        "rougeL_f": 0.6666666666666666,
        "bleu1": 0.5,
        "bleu2": 0.12909944487358058,
        "bleu3": 0.09635409769034631,
        "bleu4": 0.09554427922043669,
        "bert_precision": 0.9093739986419678,
        "bert_recall": 0.9383419752120972,
        "bert_f1": 0.9236308932304382,
        "meteor": 0.3333333333333333,
        "sbert_similarity": 0.9388758540153503
      }
    },
    {
      "sample_id": 0,
      "question": "When is Caroline's youth center putting on a talent show?",
      "prediction": "3:19 pm on 28 August, 2023",
      "reference": "September 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.25,
        "rouge1_f": 0.22222222222222224,
        "rouge2_f": 0.0,
        "rougeL_f": 0.22222222222222224,
        "bleu1": 0.14285714285714285,
        "bleu2": 0.048795003647426664,
        "bleu3": 0.037468614372419724,
        "bleu4": 0.033031643180138064,
        "bert_precision": 0.7897067666053772,
        "bert_recall": 0.867905855178833,
        "bert_f1": 0.8269618153572083,
        "meteor": 0.20833333333333334,
        "sbert_similarity": 0.6565282344818115
      }
    },
    {
      "sample_id": 0,
      "question": "Would Melanie likely enjoy the song \"The Four Seasons\" by Vivaldi?",
      "prediction": "No, Melanie would likely not enjoy the song 'The Four Seasons' by Vivaldi.",
      "reference": "Yes; it's classical music",
      "category": 3,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8548759818077087,
        "bert_recall": 0.8776440620422363,
        "bert_f1": 0.8661103844642639,
        "meteor": 0.0,
        "sbert_similarity": 0.38256001472473145
      }
    },
    {
      "sample_id": 0,
      "question": "What are some changes Caroline has faced during her transition journey?",
      "prediction": "Caroline has faced a transition and an ongoing process of exploring her body, while also learning to accept imperfections.",
      "reference": "Changes to her body, losing unsupportive friends",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.23076923076923078,
        "rouge1_f": 0.23076923076923078,
        "rouge2_f": 0.08333333333333333,
        "rougeL_f": 0.15384615384615385,
        "bleu1": 0.19047619047619047,
        "bleu2": 0.13801311186847087,
        "bleu3": 0.10241386152405135,
        "bleu4": 0.04857956790684703,
        "bert_precision": 0.8599812984466553,
        "bert_recall": 0.8636741638183594,
        "bert_f1": 0.8618237376213074,
        "meteor": 0.3116531165311653,
        "sbert_similarity": 0.3731921911239624
      }
    },
    {
      "sample_id": 0,
      "question": "What does Melanie do with her family on hikes?",
      "prediction": "family and camping",
      "reference": "Roast marshmallows, tell stories",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8445045948028564,
        "bert_recall": 0.8028994798660278,
        "bert_f1": 0.8231766819953918,
        "meteor": 0.0,
        "sbert_similarity": 0.3685782551765442
      }
    },
    {
      "sample_id": 0,
      "question": "When did Caroline go biking with friends?",
      "prediction": "13 September, 2023",
      "reference": "The weekend before 13 September 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.6666666666666666,
        "rouge1_f": 0.6666666666666666,
        "rouge2_f": 0.5714285714285715,
        "rougeL_f": 0.6666666666666666,
        "bleu1": 0.45489799478447507,
        "bleu2": 0.30326532985631666,
        "bleu3": 0.14283448174223617,
        "bleu4": 0.11404605374835301,
        "bert_precision": 0.9265691041946411,
        "bert_recall": 0.8848216533660889,
        "bert_f1": 0.9052143096923828,
        "meteor": 0.17543859649122806,
        "sbert_similarity": 0.80759596824646
      }
    },
    {
      "sample_id": 0,
      "question": "How long has Melanie been practicing art?",
      "prediction": "7 years and 1 month",
      "reference": "Since 2016",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.7639655470848083,
        "bert_recall": 0.865747332572937,
        "bert_f1": 0.8116781711578369,
        "meteor": 0.0,
        "sbert_similarity": 0.32911115884780884
      }
    },
    {
      "sample_id": 0,
      "question": "What personality traits might Melanie say Caroline has?",
      "prediction": "Melanie and Caroline are both creative, expressive, and supportive. They share a passion for nature and self-expression.",
      "reference": "Thoughtful, authentic, driven",
      "category": 3,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0.09523809523809525,
        "bleu2": 0.02182178902359924,
        "bleu3": 0.0141800886003256,
        "bleu4": 0.010862721615727716,
        "bert_precision": 0.8622358441352844,
        "bert_recall": 0.8518567681312561,
        "bert_f1": 0.8570148348808289,
        "meteor": 0.0,
        "sbert_similarity": 0.2609850764274597
      }
    },
    {
      "sample_id": 0,
      "question": "What transgender-specific events has Caroline attended?",
      "prediction": "Caroline has attended a transgender poetry reading and an LGBTQ art show.",
      "reference": "Poetry reading, conference",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.26666666666666666,
        "rouge1_f": 0.26666666666666666,
        "rouge2_f": 0.15384615384615385,
        "rougeL_f": 0.26666666666666666,
        "bleu1": 0.15384615384615385,
        "bleu2": 0.11322770341445958,
        "bleu3": 0.05034417113155098,
        "bleu4": 0.03285702044797773,
        "bert_precision": 0.8717992901802063,
        "bert_recall": 0.9000029563903809,
        "bert_f1": 0.8856766819953918,
        "meteor": 0.12820512820512822,
        "sbert_similarity": 0.4450914263725281
      }
    },
    {
      "sample_id": 0,
      "question": "What book did Melanie read from Caroline's suggestion?",
      "prediction": "The book 'The Color Purple' by Alice Walker was suggested for Melanie's reading.",
      "reference": "\"Becoming Nicole\"",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8271812796592712,
        "bert_recall": 0.8547609448432922,
        "bert_f1": 0.8407450318336487,
        "meteor": 0.0,
        "sbert_similarity": 0.17325544357299805
      }
    },
    {
      "sample_id": 0,
      "question": "When did Melanie's friend adopt a child?",
      "prediction": "On 17 July, 2023",
      "reference": 2022,
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.7666100263595581,
        "bert_recall": 0.8817809820175171,
        "bert_f1": 0.8201720714569092,
        "meteor": 0.0,
        "sbert_similarity": 0.6477062702178955
      }
    },
    {
      "sample_id": 0,
      "question": "When did Melanie get hurt?",
      "prediction": "7:55 pm on 9 June, 2023",
      "reference": "September 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.25,
        "rouge1_f": 0.22222222222222224,
        "rouge2_f": 0.0,
        "rougeL_f": 0.22222222222222224,
        "bleu1": 0.14285714285714285,
        "bleu2": 0.048795003647426664,
        "bleu3": 0.037468614372419724,
        "bleu4": 0.033031643180138064,
        "bert_precision": 0.7923839688301086,
        "bert_recall": 0.869549036026001,
        "bert_f1": 0.8291751146316528,
        "meteor": 0.20833333333333334,
        "sbert_similarity": 0.6567151546478271
      }
    },
    {
      "sample_id": 0,
      "question": "When did Melanie's family go on a roadtrip?",
      "prediction": "7:55 pm on 9 June, 2023",
      "reference": "The weekend before 20 October 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.16666666666666666,
        "rouge1_f": 0.15384615384615383,
        "rouge2_f": 0.0,
        "rougeL_f": 0.15384615384615383,
        "bleu1": 0.14285714285714285,
        "bleu2": 0.048795003647426664,
        "bleu3": 0.037468614372419724,
        "bleu4": 0.033031643180138064,
        "bert_precision": 0.8611142635345459,
        "bert_recall": 0.8633313179016113,
        "bert_f1": 0.862221360206604,
        "meteor": 0.08333333333333333,
        "sbert_similarity": 0.6757801175117493
      }
    },
    {
      "sample_id": 0,
      "question": "How many children does Melanie have?",
      "prediction": "2.0 children",
      "reference": 3,
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8637257814407349,
        "bert_recall": 0.8848049640655518,
        "bert_f1": 0.874138355255127,
        "meteor": 0.0,
        "sbert_similarity": 0.224164217710495
      }
    },
    {
      "sample_id": 0,
      "question": "When did Melanie go on a hike after the roadtrip?",
      "prediction": "10:37 am on 27 June, 2023",
      "reference": "19 October 2023",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.2222222222222222,
        "rouge1_f": 0.2,
        "rouge2_f": 0.0,
        "rougeL_f": 0.2,
        "bleu1": 0.14285714285714285,
        "bleu2": 0.048795003647426664,
        "bleu3": 0.037468614372419724,
        "bleu4": 0.033031643180138064,
        "bert_precision": 0.832482635974884,
        "bert_recall": 0.8911905288696289,
        "bert_f1": 0.8608368039131165,
        "meteor": 0.15151515151515152,
        "sbert_similarity": 0.7062749862670898
      }
    },
    {
      "sample_id": 0,
      "question": "Would Melanie go on another roadtrip soon?",
      "prediction": "Yes, Melanie is planning to take another roadtrip soon.",
      "reference": "Likely no; since this one went badly",
      "category": 3,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8665491342544556,
        "bert_recall": 0.8376652598381042,
        "bert_f1": 0.8518624901771545,
        "meteor": 0.0,
        "sbert_similarity": 0.22742748260498047
      }
    },
    {
      "sample_id": 0,
      "question": "What items has Melanie bought?",
      "prediction": "[\"necklace\"]",
      "reference": "Figurines, shoes",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8259193897247314,
        "bert_recall": 0.7767043113708496,
        "bert_f1": 0.8005561828613281,
        "meteor": 0.0,
        "sbert_similarity": 0.2543737292289734
      }
    },
    {
      "sample_id": 0,
      "question": "When did Caroline pass the adoption interview?",
      "prediction": "March 23, 2023",
      "reference": "The Friday before 22 October 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.2222222222222222,
        "rouge1_f": 0.2222222222222222,
        "rouge2_f": 0.0,
        "rougeL_f": 0.2222222222222222,
        "bleu1": 0.15163266492815836,
        "bleu2": 0.05536842069051654,
        "bleu3": 0.04649251600872798,
        "bleu4": 0.04873039689743777,
        "bert_precision": 0.9012119770050049,
        "bert_recall": 0.857352077960968,
        "bert_f1": 0.8787351250648499,
        "meteor": 0.08771929824561403,
        "sbert_similarity": 0.779310941696167
      }
    },
    {
      "sample_id": 0,
      "question": "When did Melanie buy the figurines?",
      "prediction": "13 September, 2023",
      "reference": "21 October 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.3333333333333333,
        "rouge1_f": 0.3333333333333333,
        "rouge2_f": 0.0,
        "rougeL_f": 0.3333333333333333,
        "bleu1": 0.25,
        "bleu2": 0.09128709291752769,
        "bleu3": 0.07665320007195604,
        "bleu4": 0.08034284189446518,
        "bert_precision": 0.9433450102806091,
        "bert_recall": 0.9640657305717468,
        "bert_f1": 0.953592836856842,
        "meteor": 0.16666666666666666,
        "sbert_similarity": 0.842340886592865
      }
    },
    {
      "sample_id": 0,
      "question": "Would Caroline want to move back to her home country soon?",
      "prediction": "no",
      "reference": "No; she's in the process of adopting children.",
      "category": 3,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.19999999999999998,
        "rouge2_f": 0.0,
        "rougeL_f": 0.19999999999999998,
        "bleu1": 4.5399929762484854e-05,
        "bleu2": 1.435671836611194e-05,
        "bleu3": 9.93242240643695e-06,
        "bleu4": 8.07337603138401e-06,
        "bert_precision": 0.8665685653686523,
        "bert_recall": 0.8397024869918823,
        "bert_f1": 0.8529239892959595,
        "meteor": 0.0,
        "sbert_similarity": 0.21591627597808838
      }
    },
    {
      "sample_id": 0,
      "question": "What did the charity race raise awareness for?",
      "prediction": "mental health",
      "reference": "mental health",
      "category": 4,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 0.46773514128719823,
        "bleu4": 0.316227766016838,
        "bert_precision": 1.000000238418579,
        "bert_recall": 1.000000238418579,
        "bert_f1": 1.000000238418579,
        "meteor": 0.9375,
        "sbert_similarity": 1.0000001192092896
      }
    },
    {
      "sample_id": 0,
      "question": "What did Melanie realize after the charity race?",
      "prediction": "Melanie realized that taking care of our minds is vital for mental health.",
      "reference": "self-care is important",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.125,
        "rouge1_f": 0.23529411764705882,
        "rouge2_f": 0.0,
        "rougeL_f": 0.23529411764705882,
        "bleu1": 0.07142857142857141,
        "bleu2": 0.023440361546924773,
        "bleu3": 0.017300021018422027,
        "bleu4": 0.01428363257865929,
        "bert_precision": 0.8774830102920532,
        "bert_recall": 0.9173073172569275,
        "bert_f1": 0.8969533443450928,
        "meteor": 0.125,
        "sbert_similarity": 0.4087769091129303
      }
    },
    {
      "sample_id": 0,
      "question": "How does Melanie prioritize self-care?",
      "prediction": "self-care and family",
      "reference": "by carving out some me-time each day for activities like running, reading, or playing the violin",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8507129549980164,
        "bert_recall": 0.8323996067047119,
        "bert_f1": 0.8414565920829773,
        "meteor": 0.0,
        "sbert_similarity": 0.23191800713539124
      }
    },
    {
      "sample_id": 0,
      "question": "What are Caroline's plans for the summer?",
      "prediction": "Caroline is planning a trip to the beach with her family and exploring nature, as well as organizing fun activities like campfires and hiking. She also discusses continuing her education and career options, which are exciting for the summer.",
      "reference": "researching adoption agencies",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8088680505752563,
        "bert_recall": 0.8431898951530457,
        "bert_f1": 0.8256725072860718,
        "meteor": 0.0,
        "sbert_similarity": 0.06428700685501099
      }
    },
    {
      "sample_id": 0,
      "question": "What type of individuals does the adoption agency Caroline is considering support?",
      "prediction": "LGBTQ+ individuals with inclusive support and adoption agencies.",
      "reference": "LGBTQ+ individuals",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.4,
        "rouge1_f": 0.4,
        "rouge2_f": 0.25,
        "rougeL_f": 0.4,
        "bleu1": 0.2222222222222222,
        "bleu2": 0.16666666666666669,
        "bleu3": 0.07542890820012248,
        "bleu4": 0.050712153369465586,
        "bert_precision": 0.875646710395813,
        "bert_recall": 0.9643681645393372,
        "bert_f1": 0.917868435382843,
        "meteor": 0.7211538461538463,
        "sbert_similarity": 0.7496994733810425
      }
    },
    {
      "sample_id": 0,
      "question": "Why did Caroline choose the adoption agency?",
      "prediction": "Because she wanted to create a loving home for kids who needed it, and is grateful for the support from friends and mentors.",
      "reference": "because of their inclusivity and support for LGBTQ+ individuals",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.26666666666666666,
        "rouge1_f": 0.25,
        "rouge2_f": 0.0,
        "rougeL_f": 0.18750000000000003,
        "bleu1": 0.15999999999999998,
        "bleu2": 0.025819888974716113,
        "bleu3": 0.014877148519655915,
        "bleu4": 0.010713701843513142,
        "bert_precision": 0.8456411957740784,
        "bert_recall": 0.834337592124939,
        "bert_f1": 0.8399513363838196,
        "meteor": 0.19230769230769232,
        "sbert_similarity": 0.3197815418243408
      }
    },
    {
      "sample_id": 0,
      "question": "What is Caroline excited about in the adoption process?",
      "prediction": "Caroline is excited about getting a loving home for kids who need it.",
      "reference": "creating a family for kids who need one",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.4761904761904762,
        "rouge1_f": 0.4761904761904762,
        "rouge2_f": 0.3157894736842105,
        "rougeL_f": 0.4761904761904762,
        "bleu1": 0.3571428571428572,
        "bleu2": 0.2870846258816073,
        "bleu3": 0.24293866735461883,
        "bleu4": 0.18798317647335086,
        "bert_precision": 0.8960666060447693,
        "bert_recall": 0.9105445742607117,
        "bert_f1": 0.903247594833374,
        "meteor": 0.5694117647058823,
        "sbert_similarity": 0.37909209728240967
      }
    },
    {
      "sample_id": 0,
      "question": "What does Melanie think about Caroline's decision to adopt?",
      "prediction": "Melanie thinks Caroline is doing amazing and inspiring.",
      "reference": "she thinks Caroline is doing something amazing and will be an awesome mom",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.5714285714285714,
        "rouge1_f": 0.5714285714285714,
        "rouge2_f": 0.4210526315789474,
        "rougeL_f": 0.5714285714285714,
        "bleu1": 0.4274535922866364,
        "bleu2": 0.3701856698591431,
        "bleu3": 0.2951126062698573,
        "bleu4": 0.22758576422172913,
        "bert_precision": 0.9605340957641602,
        "bert_recall": 0.9231101870536804,
        "bert_f1": 0.9414503574371338,
        "meteor": 0.4711111111111111,
        "sbert_similarity": 0.7221944332122803
      }
    },
    {
      "sample_id": 0,
      "question": "How long have Mel and her husband been married?",
      "prediction": "2 years.",
      "reference": "Mel and her husband have been married for 5 years.",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.16666666666666669,
        "rouge1_f": 0.16666666666666669,
        "rouge2_f": 0.0,
        "rougeL_f": 0.16666666666666669,
        "bleu1": 0.04632230081520103,
        "bleu2": 0.0401162892677087,
        "bleu3": 0.022616810933284073,
        "bleu4": 0.0166955629691623,
        "bert_precision": 0.9232646226882935,
        "bert_recall": 0.8425200581550598,
        "bert_f1": 0.8810462355613708,
        "meteor": 0.05434782608695652,
        "sbert_similarity": 0.23449300229549408
      }
    },
    {
      "sample_id": 0,
      "question": "What does Caroline's necklace symbolize?",
      "prediction": "Love, faith and strength.",
      "reference": "love, faith, and strength",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 0.8333333333333334,
        "bleu2": 0.7071067811865476,
        "bleu3": 0.5034777750283593,
        "bleu4": 0.25406637407730737,
        "bert_precision": 0.9868189692497253,
        "bert_recall": 0.9856107234954834,
        "bert_f1": 0.9862144589424133,
        "meteor": 0.25,
        "sbert_similarity": 0.9717916250228882
      }
    },
    {
      "sample_id": 0,
      "question": "What country is Caroline's grandma from?",
      "prediction": "Caroline's grandma is from the US.",
      "reference": "Sweden",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8227814435958862,
        "bert_recall": 0.839381754398346,
        "bert_f1": 0.8309987187385559,
        "meteor": 0.0,
        "sbert_similarity": 0.1976855993270874
      }
    },
    {
      "sample_id": 0,
      "question": "What was grandma's gift to Caroline?",
      "prediction": "A necklace from a Swedish grandmother, passed down through generations to symbolize love and strength.",
      "reference": "necklace",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.13333333333333333,
        "rouge1_f": 0.125,
        "rouge2_f": 0.0,
        "rougeL_f": 0.125,
        "bleu1": 0.0588235294117647,
        "bleu2": 0.019174124721184262,
        "bleu3": 0.014076093604378503,
        "bleu4": 0.011502783619900045,
        "bert_precision": 0.7981986999511719,
        "bert_recall": 0.8370943069458008,
        "bert_f1": 0.8171839118003845,
        "meteor": 0.20833333333333337,
        "sbert_similarity": 0.6419719457626343
      }
    },
    {
      "sample_id": 0,
      "question": "What is Melanie's hand-painted bowl a reminder of?",
      "prediction": "art and self-expression",
      "reference": "art and self-expression",
      "category": 4,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 0.5623413251903491,
        "bert_precision": 0.9999998807907104,
        "bert_recall": 0.9999998807907104,
        "bert_f1": 0.9999998807907104,
        "meteor": 0.9814814814814815,
        "sbert_similarity": 1.0000001192092896
      }
    },
    {
      "sample_id": 0,
      "question": "What did Melanie and her family do while camping?",
      "prediction": "Melanie and her family went camping with a fire pit, roasted marshmallows, and explored nature. They also took part in a hike and shared stories around the campfire.",
      "reference": "explored nature, roasted marshmallows, and went on a hike",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.47058823529411764,
        "rouge1_f": 0.43243243243243246,
        "rouge2_f": 0.22857142857142856,
        "rougeL_f": 0.2702702702702703,
        "bleu1": 0.3125,
        "bleu2": 0.2459346884189824,
        "bleu3": 0.18532800930965523,
        "bleu4": 0.1429117357407516,
        "bert_precision": 0.8910852670669556,
        "bert_recall": 0.9440851211547852,
        "bert_f1": 0.9168198704719543,
        "meteor": 0.5251825500842539,
        "sbert_similarity": 0.5738885402679443
      }
    },
    {
      "sample_id": 0,
      "question": "What kind of counseling and mental health services is Caroline interested in pursuing?",
      "prediction": "Counseling and mental health services that include support group activities, helping individuals navigate their journeys with empathy and understanding.",
      "reference": "working with trans people, helping them accept themselves and supporting their mental health",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.3870967741935484,
        "rouge1_f": 0.4375,
        "rouge2_f": 0.06666666666666667,
        "rougeL_f": 0.18749999999999997,
        "bleu1": 0.3333333333333333,
        "bleu2": 0.1825741858350554,
        "bleu3": 0.057618413718129605,
        "bleu4": 0.031420504922717064,
        "bert_precision": 0.8616369366645813,
        "bert_recall": 0.8759665489196777,
        "bert_f1": 0.8687427043914795,
        "meteor": 0.3526410564225691,
        "sbert_similarity": 0.5494664907455444
      }
    },
    {
      "sample_id": 0,
      "question": "What workshop did Caroline attend recently?",
      "prediction": "The workshop was a nature-inspired art show featuring painting of a sunset and palm tree.",
      "reference": "LGBTQ+ counseling workshop",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.11764705882352941,
        "rouge1_f": 0.10526315789473684,
        "rouge2_f": 0.0,
        "rougeL_f": 0.10526315789473684,
        "bleu1": 0.0625,
        "bleu2": 0.02041241452319315,
        "bleu3": 0.015007489164383502,
        "bleu4": 0.012300686288463768,
        "bert_precision": 0.8539605140686035,
        "bert_recall": 0.8373496532440186,
        "bert_f1": 0.8455734848976135,
        "meteor": 0.11904761904761904,
        "sbert_similarity": 0.17127516865730286
      }
    },
    {
      "sample_id": 0,
      "question": "What was discussed in the LGBTQ+ counseling workshop?",
      "prediction": "they talked about different therapeutic methods and how to best work with trans people.",
      "reference": "therapeutic methods and how to best work with trans people",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.8333333333333333,
        "rouge1_f": 0.8333333333333333,
        "rouge2_f": 0.8181818181818181,
        "rougeL_f": 0.8333333333333333,
        "bleu1": 0.6666666666666666,
        "bleu2": 0.6546536707079771,
        "bleu3": 0.6441485748898267,
        "bleu4": 0.6262844962765469,
        "bert_precision": 0.9291307330131531,
        "bert_recall": 0.9493020176887512,
        "bert_f1": 0.9391080737113953,
        "meteor": 0.8647910731244065,
        "sbert_similarity": 0.8247992396354675
      }
    },
    {
      "sample_id": 0,
      "question": "What motivated Caroline to pursue counseling?",
      "prediction": "Caroline motivated herself to pursue counseling by prioritizing her mental health and seeking support from loved ones, which she believed would help her navigate and overcome challenges with confidence.",
      "reference": "her own journey and the support she received, and how counseling improved her life",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.2564102564102564,
        "rouge1_f": 0.32558139534883723,
        "rouge2_f": 0.0,
        "rougeL_f": 0.23255813953488377,
        "bleu1": 0.25806451612903225,
        "bleu2": 0.029329423004270664,
        "bleu3": 0.014990944053783671,
        "bleu4": 0.010145249973866257,
        "bert_precision": 0.8735992312431335,
        "bert_recall": 0.8957760334014893,
        "bert_f1": 0.8845486044883728,
        "meteor": 0.22580645161290322,
        "sbert_similarity": 0.6459678411483765
      }
    },
    {
      "sample_id": 0,
      "question": "What kind of place does Caroline want to create for people?",
      "prediction": "A safe and loving home for children, emphasizing love and acceptance as fundamental rights.",
      "reference": "a safe and inviting place for people to grow",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.3636363636363637,
        "rouge1_f": 0.34782608695652173,
        "rouge2_f": 0.1904761904761905,
        "rougeL_f": 0.34782608695652173,
        "bleu1": 0.25,
        "bleu2": 0.1825741858350554,
        "bleu3": 0.13624724651304662,
        "bleu4": 0.06541868941754436,
        "bert_precision": 0.8888559341430664,
        "bert_recall": 0.9048672914505005,
        "bert_f1": 0.8967901468276978,
        "meteor": 0.4694736842105264,
        "sbert_similarity": 0.48991337418556213
      }
    },
    {
      "sample_id": 0,
      "question": "Did Melanie make the black and white bowl in the photo?",
      "prediction": "Yes, Mel made the black and white bowl in the photo.",
      "reference": "Yes",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.18181818181818182,
        "rouge1_f": 0.16666666666666669,
        "rouge2_f": 0.0,
        "rougeL_f": 0.16666666666666669,
        "bleu1": 0.07692307692307693,
        "bleu2": 0.025318484177091663,
        "bleu3": 0.018733084686676876,
        "bleu4": 0.01553712569276035,
        "bert_precision": 0.8194844722747803,
        "bert_recall": 0.8486971259117126,
        "bert_f1": 0.8338350057601929,
        "meteor": 0.0,
        "sbert_similarity": 0.17181538045406342
      }
    },
    {
      "sample_id": 0,
      "question": "What kind of books does Caroline have in her library?",
      "prediction": "Caroline has a collection of books including classics, stories from different cultures, and educational materials.",
      "reference": "kids' books - classics, stories from different cultures, educational books",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.5833333333333334,
        "rouge1_f": 0.5833333333333334,
        "rouge2_f": 0.36363636363636365,
        "rougeL_f": 0.5833333333333334,
        "bleu1": 0.5,
        "bleu2": 0.420084025208403,
        "bleu3": 0.38432831084712576,
        "bleu4": 0.34823528327578535,
        "bert_precision": 0.9306110143661499,
        "bert_recall": 0.933864414691925,
        "bert_f1": 0.9322348237037659,
        "meteor": 0.640427599611273,
        "sbert_similarity": 0.6837618350982666
      }
    },
    {
      "sample_id": 0,
      "question": "What was Melanie's favorite book from her childhood?",
      "prediction": "Charlotte's Web",
      "reference": "\"Charlotte's Web\"",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 0.513417119032592,
        "bleu2": 0.513417119032592,
        "bleu3": 0.513417119032592,
        "bleu4": 0.288715663092199,
        "bert_precision": 0.8835268020629883,
        "bert_recall": 0.8591200113296509,
        "bert_f1": 0.8711524605751038,
        "meteor": 0.0,
        "sbert_similarity": 0.954933226108551
      }
    },
    {
      "sample_id": 0,
      "question": "What book did Caroline recommend to Melanie?",
      "prediction": "A book about finding your identity and self-acceptance that helped her navigate her transition journey.",
      "reference": "\"Becoming Nicole\"",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8188862204551697,
        "bert_recall": 0.8342469930648804,
        "bert_f1": 0.8264952898025513,
        "meteor": 0.0,
        "sbert_similarity": 0.2891165614128113
      }
    },
    {
      "sample_id": 0,
      "question": "What did Caroline take away from the book \"Becoming Nicole\"?",
      "prediction": "Becoming Nicole is a book that helps readers understand themselves and explore their true potential, which emphasizes the importance of self-discovery and personal growth.",
      "reference": "Lessons on self-acceptance and finding support",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.06896551724137931,
        "rouge1_f": 0.125,
        "rouge2_f": 0.0,
        "rougeL_f": 0.125,
        "bleu1": 0.03846153846153846,
        "bleu2": 0.012403473458920851,
        "bleu3": 0.009042091188715478,
        "bleu4": 0.0072658577559704465,
        "bert_precision": 0.8494726419448853,
        "bert_recall": 0.8758232593536377,
        "bert_f1": 0.8624467253684998,
        "meteor": 0.06410256410256411,
        "sbert_similarity": 0.2635919451713562
      }
    },
    {
      "sample_id": 0,
      "question": "What are the new shoes that Melanie got used for?",
      "prediction": "sneakers",
      "reference": "Running",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.9985332489013672,
        "bert_recall": 0.9985332489013672,
        "bert_f1": 0.9985332489013672,
        "meteor": 0.0,
        "sbert_similarity": 0.36407670378685
      }
    },
    {
      "sample_id": 0,
      "question": "What is Melanie's reason for getting into running?",
      "prediction": "Melanie's reason for getting into running is to destress and clear her mind.",
      "reference": "To de-stress and clear her mind",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.5263157894736842,
        "rouge1_f": 0.4761904761904762,
        "rouge2_f": 0.3157894736842105,
        "rougeL_f": 0.4761904761904762,
        "bleu1": 0.3333333333333333,
        "bleu2": 0.26726124191242434,
        "bleu3": 0.22569209116099667,
        "bleu4": 0.17395797375642233,
        "bert_precision": 0.8800427913665771,
        "bert_recall": 0.9198237657546997,
        "bert_f1": 0.8994936943054199,
        "meteor": 0.5597014925373134,
        "sbert_similarity": 0.44855260848999023
      }
    },
    {
      "sample_id": 0,
      "question": "What does Melanie say running has been great for?",
      "prediction": "Melanie says that running has been great for helping you express yourself and create memories.",
      "reference": "Her mental health",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8342185616493225,
        "bert_recall": 0.8395102620124817,
        "bert_f1": 0.8368560075759888,
        "meteor": 0.0,
        "sbert_similarity": 0.2355644702911377
      }
    },
    {
      "sample_id": 0,
      "question": "What did Mel and her kids make during the pottery workshop?",
      "prediction": "They made clay sculptures and bowls with flower designs.",
      "reference": "pots",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8154801726341248,
        "bert_recall": 0.8739178776741028,
        "bert_f1": 0.8436883091926575,
        "meteor": 0.0,
        "sbert_similarity": 0.3157898485660553
      }
    },
    {
      "sample_id": 0,
      "question": "What kind of pot did Mel and her kids make with clay?",
      "prediction": "A bowl made of pottery with a black and white flower design.",
      "reference": "a cup with a dog face on it",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.2222222222222222,
        "rouge1_f": 0.3,
        "rouge2_f": 0.1111111111111111,
        "rougeL_f": 0.3,
        "bleu1": 0.23076923076923078,
        "bleu2": 0.1386750490563073,
        "bleu3": 0.057551852907951895,
        "bleu4": 0.036362270465000714,
        "bert_precision": 0.8825488090515137,
        "bert_recall": 0.9048001170158386,
        "bert_f1": 0.8935359716415405,
        "meteor": 0.30423280423280424,
        "sbert_similarity": 0.23911668360233307
      }
    },
    {
      "sample_id": 0,
      "question": "What creative project do Mel and her kids do together besides pottery?",
      "prediction": "clay sculptures and art projects with children",
      "reference": "painting",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8030959963798523,
        "bert_recall": 0.8685663938522339,
        "bert_f1": 0.8345491290092468,
        "meteor": 0.0,
        "sbert_similarity": 0.4063016176223755
      }
    },
    {
      "sample_id": 0,
      "question": "What did Mel and her kids paint in their latest project in July 2023?",
      "prediction": "Mel and her kids painted a clay sculpture with a dog face on it.",
      "reference": "a sunset with a palm tree",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.2222222222222222,
        "rouge1_f": 0.3,
        "rouge2_f": 0.1111111111111111,
        "rougeL_f": 0.3,
        "bleu1": 0.2,
        "bleu2": 0.11952286093343935,
        "bleu3": 0.04937604958709526,
        "bleu4": 0.030934588294313718,
        "bert_precision": 0.8701767921447754,
        "bert_recall": 0.883010983467102,
        "bert_f1": 0.8765469193458557,
        "meteor": 0.37581699346405234,
        "sbert_similarity": 0.08202965557575226
      }
    },
    {
      "sample_id": 0,
      "question": "What did Caroline see at the council meeting for adoption?",
      "prediction": "Caroline saw a council meeting where she met diverse individuals who wanted to create loving homes for children in need.",
      "reference": "many people wanting to create loving homes for children in need",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.5161290322580645,
        "rouge1_f": 0.5806451612903226,
        "rouge2_f": 0.5517241379310345,
        "rougeL_f": 0.5806451612903226,
        "bleu1": 0.38095238095238093,
        "bleu2": 0.36514837167011066,
        "bleu3": 0.35158554184163054,
        "bleu4": 0.3288580454955831,
        "bert_precision": 0.9006938934326172,
        "bert_recall": 0.9475488066673279,
        "bert_f1": 0.9235274195671082,
        "meteor": 0.6716123949579832,
        "sbert_similarity": 0.5778934955596924
      }
    },
    {
      "sample_id": 0,
      "question": "What do sunflowers represent according to Caroline?",
      "prediction": "Sunflowers mean warmth and happiness, right? while roses stand for love and beauty?",
      "reference": "warmth and happiness",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.4,
        "rouge1_f": 0.375,
        "rouge2_f": 0.2857142857142857,
        "rougeL_f": 0.375,
        "bleu1": 0.1875,
        "bleu2": 0.15811388300841897,
        "bleu3": 0.12390760171084929,
        "bleu4": 0.06087895024805787,
        "bert_precision": 0.8104417324066162,
        "bert_recall": 0.8732234239578247,
        "bert_f1": 0.8406620025634766,
        "meteor": 0.25,
        "sbert_similarity": 0.435322642326355
      }
    },
    {
      "sample_id": 0,
      "question": "Why are flowers important to Melanie?",
      "prediction": "Because they represent growth, beauty, and remind us to appreciate small moments.",
      "reference": "They remind her to appreciate the small moments and were a part of her wedding decor",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.5185185185185186,
        "rouge1_f": 0.5,
        "rouge2_f": 0.15384615384615383,
        "rougeL_f": 0.42857142857142855,
        "bleu1": 0.4365699263480883,
        "bleu2": 0.24154686488587782,
        "bleu3": 0.07679547178644844,
        "bleu4": 0.04253467024029704,
        "bert_precision": 0.9037790298461914,
        "bert_recall": 0.8761621713638306,
        "bert_f1": 0.8897563815116882,
        "meteor": 0.27332621082621084,
        "sbert_similarity": 0.523457407951355
      }
    },
    {
      "sample_id": 0,
      "question": "What inspired Caroline's painting for the art show?",
      "prediction": "inspired by her own experiences and the themes of unity and strength in her community.",
      "reference": "visiting an LGBTQ center and wanting to capture unity and strength",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.2608695652173913,
        "rouge1_f": 0.30769230769230765,
        "rouge2_f": 0.16666666666666666,
        "rougeL_f": 0.30769230769230765,
        "bleu1": 0.25,
        "bleu2": 0.1825741858350554,
        "bleu3": 0.13624724651304662,
        "bleu4": 0.06541868941754436,
        "bert_precision": 0.8923817873001099,
        "bert_recall": 0.883468747138977,
        "bert_f1": 0.8879029154777527,
        "meteor": 0.3289473684210526,
        "sbert_similarity": 0.4283674955368042
      }
    },
    {
      "sample_id": 0,
      "question": "How often does Melanie go to the beach with her kids?",
      "prediction": "Usually once or twice a year. But those times are always special to spend time together and chill.",
      "reference": "once or twice a year",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.4347826086956522,
        "rouge1_f": 0.4347826086956522,
        "rouge2_f": 0.38095238095238093,
        "rougeL_f": 0.4347826086956522,
        "bleu1": 0.25,
        "bleu2": 0.2294157338705618,
        "bleu3": 0.20951802859630908,
        "bleu4": 0.17923344640485428,
        "bert_precision": 0.8545583486557007,
        "bert_recall": 0.9256296157836914,
        "bert_f1": 0.8886752724647522,
        "meteor": 0.6299603174603176,
        "sbert_similarity": 0.6472978591918945
      }
    },
    {
      "sample_id": 0,
      "question": "What did Melanie and her family see during their camping trip last year?",
      "prediction": "Melanie and her family went camping in the forest, roasting marshmallows around a campfire and hiking. They explored nature, enjoyed the view from the top, and shared moments with their children.",
      "reference": "Perseid meteor shower",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.7960872650146484,
        "bert_recall": 0.7678672075271606,
        "bert_f1": 0.7817226648330688,
        "meteor": 0.0,
        "sbert_similarity": -0.03565456345677376
      }
    },
    {
      "sample_id": 0,
      "question": "How did Melanie feel while watching the meteor shower?",
      "prediction": "Melanie felt awe and wonder while watching the meteor shower.",
      "reference": "in awe of the universe",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.26666666666666666,
        "rouge1_f": 0.26666666666666666,
        "rouge2_f": 0.0,
        "rougeL_f": 0.26666666666666666,
        "bleu1": 0.18181818181818182,
        "bleu2": 0.042640143271122095,
        "bleu3": 0.028234511901075607,
        "bleu4": 0.022416933501922302,
        "bert_precision": 0.878096342086792,
        "bert_recall": 0.901416540145874,
        "bert_f1": 0.8896036148071289,
        "meteor": 0.18181818181818182,
        "sbert_similarity": 0.4369242489337921
      }
    },
    {
      "sample_id": 0,
      "question": "Whose birthday did Melanie celebrate recently?",
      "prediction": "Melanie's birthday was celebrated recently.",
      "reference": "Melanie's daughter",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.28571428571428575,
        "rouge1_f": 0.4444444444444444,
        "rouge2_f": 0.28571428571428575,
        "rougeL_f": 0.4444444444444444,
        "bleu1": 0.2857142857142857,
        "bleu2": 0.2182178902359924,
        "bleu3": 0.1006949130683619,
        "bleu4": 0.06985342056580097,
        "bert_precision": 0.867982029914856,
        "bert_recall": 0.8851499557495117,
        "bert_f1": 0.8764819502830505,
        "meteor": 0.21739130434782608,
        "sbert_similarity": 0.659034252166748
      }
    },
    {
      "sample_id": 0,
      "question": "Who performed at the concert at Melanie's daughter's birthday?",
      "prediction": "Melanie and the band played at the concert.",
      "reference": "Matt Patterson",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8355886936187744,
        "bert_recall": 0.8282730579376221,
        "bert_f1": 0.8319147825241089,
        "meteor": 0.0,
        "sbert_similarity": 0.009833105839788914
      }
    },
    {
      "sample_id": 0,
      "question": "Why did Melanie choose to use colors and patterns in her pottery project?",
      "prediction": "because it helped express her emotions and show her style in a creative way",
      "reference": "She wanted to catch the eye and make people smile.",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.08695652173913043,
        "rouge1_f": 0.08333333333333333,
        "rouge2_f": 0.0,
        "rougeL_f": 0.08333333333333333,
        "bleu1": 0.07142857142857141,
        "bleu2": 0.023440361546924773,
        "bleu3": 0.017300021018422027,
        "bleu4": 0.01428363257865929,
        "bert_precision": 0.885506272315979,
        "bert_recall": 0.8995542526245117,
        "bert_f1": 0.8924750089645386,
        "meteor": 0.04807692307692308,
        "sbert_similarity": 0.628722071647644
      }
    },
    {
      "sample_id": 0,
      "question": "What pet does Caroline have?",
      "prediction": "a dog",
      "reference": "guinea pig",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.885839581489563,
        "bert_recall": 0.8197524547576904,
        "bert_f1": 0.8515156507492065,
        "meteor": 0.0,
        "sbert_similarity": 0.4114709496498108
      }
    },
    {
      "sample_id": 0,
      "question": "What pets does Melanie have?",
      "prediction": "Bailey and Oliver",
      "reference": "Two cats and a dog",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.25,
        "rouge1_f": 0.25,
        "rouge2_f": 0.0,
        "rougeL_f": 0.25,
        "bleu1": 0.17113903967753066,
        "bleu2": 0.06628186505570068,
        "bleu3": 0.0621842545921536,
        "bleu4": 0.05833544737207805,
        "bert_precision": 0.8285648822784424,
        "bert_recall": 0.8313772082328796,
        "bert_f1": 0.8299686908721924,
        "meteor": 0.10416666666666666,
        "sbert_similarity": 0.3356642723083496
      }
    },
    {
      "sample_id": 0,
      "question": "Where did Oliver hide his bone once?",
      "prediction": "The bone was hidden in the tree in the park.",
      "reference": "In Melanie's slipper",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.2,
        "rouge1_f": 0.14285714285714288,
        "rouge2_f": 0.0,
        "rougeL_f": 0.14285714285714288,
        "bleu1": 0.0909090909090909,
        "bleu2": 0.03015113445777637,
        "bleu3": 0.022461584318318094,
        "bleu4": 0.018850319022747353,
        "bert_precision": 0.8535767793655396,
        "bert_recall": 0.8372418880462646,
        "bert_f1": 0.8453304171562195,
        "meteor": 0.13513513513513514,
        "sbert_similarity": 0.19255733489990234
      }
    },
    {
      "sample_id": 0,
      "question": "What activity did Caroline used to do with her dad?",
      "prediction": "Caroline used to do camping with her dad.",
      "reference": "Horseback riding",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8362380266189575,
        "bert_recall": 0.7895627021789551,
        "bert_f1": 0.8122303485870361,
        "meteor": 0.0,
        "sbert_similarity": 0.10911307483911514
      }
    },
    {
      "sample_id": 0,
      "question": "What did Caroline make for a local church?",
      "prediction": "a photo of a family sitting around a campfire on the beach.",
      "reference": "a stained glass window",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.14285714285714288,
        "rouge1_f": 0.125,
        "rouge2_f": 0.0,
        "rougeL_f": 0.125,
        "bleu1": 0.07692307692307693,
        "bleu2": 0.025318484177091663,
        "bleu3": 0.018733084686676876,
        "bleu4": 0.01553712569276035,
        "bert_precision": 0.8613935708999634,
        "bert_recall": 0.8504403233528137,
        "bert_f1": 0.855881929397583,
        "meteor": 0.10416666666666667,
        "sbert_similarity": 0.12593653798103333
      }
    },
    {
      "sample_id": 0,
      "question": "What did Caroline find in her neighborhood during her walk?",
      "prediction": "a red bike and a blue bike",
      "reference": "a rainbow sidewalk",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.25,
        "rouge1_f": 0.2,
        "rouge2_f": 0.0,
        "rougeL_f": 0.2,
        "bleu1": 0.14285714285714285,
        "bleu2": 0.048795003647426664,
        "bleu3": 0.037468614372419724,
        "bleu4": 0.033031643180138064,
        "bert_precision": 0.8538950681686401,
        "bert_recall": 0.877501368522644,
        "bert_f1": 0.8655372858047485,
        "meteor": 0.14705882352941177,
        "sbert_similarity": 0.18356363475322723
      }
    },
    {
      "sample_id": 0,
      "question": "Which song motivates Caroline to be courageous?",
      "prediction": "Brave by Sara Bareilles.",
      "reference": "Brave by Sara Bareilles",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 0.8,
        "bleu2": 0.7745966692414834,
        "bleu3": 0.7390601692542802,
        "bleu4": 0.668740304976422,
        "bert_precision": 0.9914209246635437,
        "bert_recall": 0.9897130727767944,
        "bert_f1": 0.9905663132667542,
        "meteor": 0.7361111111111112,
        "sbert_similarity": 0.9866514801979065
      }
    },
    {
      "sample_id": 0,
      "question": "Which  classical musicians does Melanie enjoy listening to?",
      "prediction": "Melanie enjoys classical music.",
      "reference": "Bach and Mozart",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.873189389705658,
        "bert_recall": 0.8367842435836792,
        "bert_f1": 0.8545992970466614,
        "meteor": 0.0,
        "sbert_similarity": 0.3825531601905823
      }
    },
    {
      "sample_id": 0,
      "question": "Who is Melanie a fan of in terms of modern music?",
      "prediction": "Ed Sheeran's 'Perfect' and classical music like Bach and Mozart.",
      "reference": "Ed Sheeran",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.1818181818181818,
        "rouge1_f": 0.3076923076923077,
        "rouge2_f": 0.18181818181818182,
        "rougeL_f": 0.3076923076923077,
        "bleu1": 0.15384615384615385,
        "bleu2": 0.11322770341445958,
        "bleu3": 0.05034417113155098,
        "bleu4": 0.03285702044797773,
        "bert_precision": 0.8415502905845642,
        "bert_recall": 0.9698333144187927,
        "bert_f1": 0.9011492133140564,
        "meteor": 0.17857142857142858,
        "sbert_similarity": 0.5927196741104126
      }
    },
    {
      "sample_id": 0,
      "question": "How long has Melanie been creating art?",
      "prediction": "Seven years",
      "reference": "7 years",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.5,
        "rouge1_f": 0.5,
        "rouge2_f": 0.0,
        "rougeL_f": 0.5,
        "bleu1": 0.5,
        "bleu2": 0.223606797749979,
        "bleu3": 0.1740444189610778,
        "bleu4": 0.1495348781221221,
        "bert_precision": 0.9971786737442017,
        "bert_recall": 0.9971786737442017,
        "bert_f1": 0.9971786737442017,
        "meteor": 0.9375,
        "sbert_similarity": 0.9557746052742004
      }
    },
    {
      "sample_id": 0,
      "question": "What precautionary sign did Melanie see at the caf\u00e9?",
      "prediction": "thoughtful signs",
      "reference": "A sign stating that someone is not being able to leave",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.15384615384615385,
        "rouge2_f": 0.0,
        "rougeL_f": 0.15384615384615385,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8847063779830933,
        "bert_recall": 0.8190060257911682,
        "bert_f1": 0.850589394569397,
        "meteor": 0.04950495049504951,
        "sbert_similarity": 0.3899078965187073
      }
    },
    {
      "sample_id": 0,
      "question": "What advice does Caroline give for getting started with adoption?",
      "prediction": "Caroline advises researching adoption agencies and preparing for the emotional process to ensure a safe, loving home for kids.",
      "reference": "Do research, find an adoption agency or lawyer, gather necessary documents, and prepare emotionally.",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.125,
        "rouge1_f": 0.36363636363636365,
        "rouge2_f": 0.12903225806451615,
        "rougeL_f": 0.36363636363636365,
        "bleu1": 0.19047619047619047,
        "bleu2": 0.030860669992418384,
        "bleu3": 0.01782456102251381,
        "bleu4": 0.01291802583371725,
        "bert_precision": 0.8880287408828735,
        "bert_recall": 0.8964419364929199,
        "bert_f1": 0.8922155499458313,
        "meteor": 0.25862068965517243,
        "sbert_similarity": 0.6040457487106323
      }
    },
    {
      "sample_id": 0,
      "question": "What setback did Melanie face in October 2023?",
      "prediction": "Melanie faced a setback where she had to cancel her event in October 2023 due to unexpected weather conditions.",
      "reference": "She got hurt and had to take a break from pottery.",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.27586206896551724,
        "rouge1_f": 0.26666666666666666,
        "rouge2_f": 0.07142857142857142,
        "rougeL_f": 0.19999999999999998,
        "bleu1": 0.25,
        "bleu2": 0.11470786693528089,
        "bleu3": 0.04316100889854193,
        "bleu4": 0.02560744480557418,
        "bert_precision": 0.8769152164459229,
        "bert_recall": 0.901774525642395,
        "bert_f1": 0.8891711831092834,
        "meteor": 0.1694915254237288,
        "sbert_similarity": 0.23181122541427612
      }
    },
    {
      "sample_id": 0,
      "question": "What does Melanie do to keep herself busy during her pottery break?",
      "prediction": "Melanie uses pottery to keep herself busy during her transition, exploring art creation and self-expression.",
      "reference": "Read a book and paint.",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.1,
        "rouge1_f": 0.09523809523809523,
        "rouge2_f": 0.0,
        "rougeL_f": 0.09523809523809523,
        "bleu1": 0.11764705882352941,
        "bleu2": 0.027116307227332027,
        "bleu3": 0.017693837921724947,
        "bleu4": 0.013679192123121896,
        "bert_precision": 0.8468875885009766,
        "bert_recall": 0.8648248910903931,
        "bert_f1": 0.855762243270874,
        "meteor": 0.08333333333333336,
        "sbert_similarity": 0.24427485466003418
      }
    },
    {
      "sample_id": 0,
      "question": "What painting did Melanie show to Caroline on October 13, 2023?",
      "prediction": "A photo of a plane flying in the sky with a star filled sky showing the Perseid meteor shower.",
      "reference": "A painting inspired by sunsets with a pink sky.",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.26086956521739135,
        "rouge1_f": 0.2857142857142857,
        "rouge2_f": 0.07692307692307691,
        "rougeL_f": 0.2857142857142857,
        "bleu1": 0.25,
        "bleu2": 0.11470786693528089,
        "bleu3": 0.04316100889854193,
        "bleu4": 0.02560744480557418,
        "bert_precision": 0.8659104108810425,
        "bert_recall": 0.8910489678382874,
        "bert_f1": 0.8782998323440552,
        "meteor": 0.25555555555555554,
        "sbert_similarity": 0.45728567242622375
      }
    },
    {
      "sample_id": 0,
      "question": "What kind of painting did Caroline share with Melanie on October 13, 2023?",
      "prediction": "a painting of a sunset with a pink sky",
      "reference": "An abstract painting with blue streaks on a wall.",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.375,
        "rouge1_f": 0.3333333333333333,
        "rouge2_f": 0.0,
        "rougeL_f": 0.3333333333333333,
        "bleu1": 0.2982797722714565,
        "bleu2": 0.057761629525894614,
        "bleu3": 0.03609050795723058,
        "bleu4": 0.028240990488565418,
        "bert_precision": 0.9109268188476562,
        "bert_recall": 0.9126871824264526,
        "bert_f1": 0.9118061065673828,
        "meteor": 0.16666666666666666,
        "sbert_similarity": 0.5040774941444397
      }
    },
    {
      "sample_id": 0,
      "question": "What was the poetry reading that Caroline attended about?",
      "prediction": "Caroline read a poem on the importance of learning and exploring through a trip.",
      "reference": "It was a transgender poetry reading where transgender people shared their stories.",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.08333333333333334,
        "rouge1_f": 0.15384615384615383,
        "rouge2_f": 0.0,
        "rougeL_f": 0.07692307692307691,
        "bleu1": 0.13333333333333336,
        "bleu2": 0.030860669992418384,
        "bleu3": 0.020202551616772847,
        "bleu4": 0.015718877363021206,
        "bert_precision": 0.8864182233810425,
        "bert_recall": 0.8798654675483704,
        "bert_f1": 0.8831296563148499,
        "meteor": 0.0819672131147541,
        "sbert_similarity": 0.3970947265625
      }
    },
    {
      "sample_id": 0,
      "question": "What did the posters at the poetry reading say?",
      "prediction": "The posters were filled with beautiful illustrations and messages about creativity and self-expression.",
      "reference": "\"Trans Lives Matter\"",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.837591290473938,
        "bert_recall": 0.8250483274459839,
        "bert_f1": 0.8312724828720093,
        "meteor": 0.125,
        "sbert_similarity": 0.14576418697834015
      }
    },
    {
      "sample_id": 0,
      "question": "What does Caroline's drawing symbolize for her?",
      "prediction": "art as a tool to express self-discovery and authenticity, emphasizing the beauty in imperfections and individuality, aimed at highlighting personal expression and emotional well-being.",
      "reference": "Freedom and being true to herself.",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.14285714285714288,
        "rouge1_f": 0.1875,
        "rouge2_f": 0.0,
        "rougeL_f": 0.125,
        "bleu1": 0.11111111111111109,
        "bleu2": 0.02067245576486808,
        "bleu3": 0.012497943706942991,
        "bleu4": 0.009186672262412125,
        "bert_precision": 0.8378980159759521,
        "bert_recall": 0.8816385269165039,
        "bert_f1": 0.8592119216918945,
        "meteor": 0.12820512820512822,
        "sbert_similarity": 0.35038045048713684
      }
    },
    {
      "sample_id": 0,
      "question": "How do Melanie and Caroline describe their journey through life together?",
      "prediction": "Melanie and Caroline describe their journey through life as one of inspiration, connection, and shared growth.",
      "reference": "An ongoing adventure of learning and growing.",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.18181818181818182,
        "rouge1_f": 0.17391304347826086,
        "rouge2_f": 0.0,
        "rougeL_f": 0.17391304347826086,
        "bleu1": 0.15789473684210523,
        "bleu2": 0.029617443887954613,
        "bleu3": 0.017995886897626243,
        "bleu4": 0.01340082578177889,
        "bert_precision": 0.8799105286598206,
        "bert_recall": 0.9030290246009827,
        "bert_f1": 0.8913198709487915,
        "meteor": 0.12658227848101264,
        "sbert_similarity": 0.2926204800605774
      }
    },
    {
      "sample_id": 0,
      "question": "What happened to Melanie's son on their road trip?",
      "prediction": "Melanie's son was taken to the museum where he learned about the importance of exploring and having fun together with his family. They explored nature, roasted marshmallows around the campfire, and went on a hike. The view from the top was amazing! The 2 younger kids love nature.",
      "reference": "He got into an accident",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.04444444444444445,
        "rouge1_f": 0.03703703703703703,
        "rouge2_f": 0.0,
        "rougeL_f": 0.03703703703703703,
        "bleu1": 0.018181818181818177,
        "bleu2": 0.005802588531856593,
        "bleu3": 0.00421676438118695,
        "bleu4": 0.003324612889947367,
        "bert_precision": 0.8281387090682983,
        "bert_recall": 0.8586846590042114,
        "bert_f1": 0.8431351184844971,
        "meteor": 0.05376344086021506,
        "sbert_similarity": 0.12472657859325409
      }
    },
    {
      "sample_id": 0,
      "question": "How did Melanie's son handle the accident?",
      "prediction": "Melanie's son handled the accident with care and support through his family's presence, showing that love and connection play a vital role in the event.",
      "reference": "He was scared but reassured by his family",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.06451612903225806,
        "rouge1_f": 0.11428571428571428,
        "rouge2_f": 0.060606060606060615,
        "rougeL_f": 0.11428571428571428,
        "bleu1": 0.06896551724137931,
        "bleu2": 0.049629166698546515,
        "bleu3": 0.02171882703825944,
        "bleu4": 0.013686241957588646,
        "bert_precision": 0.8600165843963623,
        "bert_recall": 0.8837401866912842,
        "bert_f1": 0.8717170357704163,
        "meteor": 0.05154639175257732,
        "sbert_similarity": 0.3208230137825012
      }
    },
    {
      "sample_id": 0,
      "question": "How did Melanie feel about her family after the accident?",
      "prediction": "Melanie felt grateful and valued by her family after the accident.",
      "reference": "They are important and mean the world to her",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.3,
        "rouge1_f": 0.3,
        "rouge2_f": 0.0,
        "rougeL_f": 0.19999999999999998,
        "bleu1": 0.25,
        "bleu2": 0.047673129462279626,
        "bleu3": 0.02935354896563489,
        "bleu4": 0.022416933501922302,
        "bert_precision": 0.9038392305374146,
        "bert_recall": 0.888253927230835,
        "bert_f1": 0.8959788084030151,
        "meteor": 0.16304347826086954,
        "sbert_similarity": 0.3220086693763733
      }
    },
    {
      "sample_id": 0,
      "question": "How did Melanie's children handle the accident?",
      "prediction": "Melanie's children handled the accident with joy and love, bringing them happiness and support.",
      "reference": "They were scared but resilient",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8645805716514587,
        "bert_recall": 0.8734369277954102,
        "bert_f1": 0.8689861297607422,
        "meteor": 0.0,
        "sbert_similarity": 0.2324398159980774
      }
    },
    {
      "sample_id": 0,
      "question": "How did Melanie feel after the accident?",
      "prediction": "Melanie felt relieved after the accident.",
      "reference": "Grateful and thankful for her family",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8891894817352295,
        "bert_recall": 0.851813554763794,
        "bert_f1": 0.8701003193855286,
        "meteor": 0.0,
        "sbert_similarity": 0.3920835852622986
      }
    },
    {
      "sample_id": 0,
      "question": "What was Melanie's reaction to her children enjoying the Grand Canyon?",
      "prediction": "Melanie says her children enjoyed the Grand Canyon a lot!",
      "reference": "She was happy and thankful",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8631848692893982,
        "bert_recall": 0.8662056922912598,
        "bert_f1": 0.8646925687789917,
        "meteor": 0.0,
        "sbert_similarity": 0.30135005712509155
      }
    },
    {
      "sample_id": 0,
      "question": "What do Melanie's family give her?",
      "prediction": "Melanie's family gives her time with the kids.",
      "reference": "Strength and motivation",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8442996740341187,
        "bert_recall": 0.8407039642333984,
        "bert_f1": 0.8424980044364929,
        "meteor": 0.0,
        "sbert_similarity": 0.11018708348274231
      }
    },
    {
      "sample_id": 0,
      "question": "How did Melanie feel about her family supporting her?",
      "prediction": "Melanie felt grateful for the support and love her family provided.",
      "reference": "She appreciated them a lot",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8684940338134766,
        "bert_recall": 0.9041441679000854,
        "bert_f1": 0.885960578918457,
        "meteor": 0.0,
        "sbert_similarity": 0.481130450963974
      }
    },
    {
      "sample_id": 0,
      "question": "What did Melanie do after the road trip to relax?",
      "prediction": "relax with the kids and explore nature at a campfire",
      "reference": "Went on a nature walk or hike",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.23529411764705882,
        "rouge1_f": 0.23529411764705882,
        "rouge2_f": 0.0,
        "rougeL_f": 0.11764705882352941,
        "bleu1": 0.2,
        "bleu2": 0.04714045207910317,
        "bleu3": 0.03136318607896979,
        "bleu4": 0.025098621243978974,
        "bert_precision": 0.8735026121139526,
        "bert_recall": 0.8798487186431885,
        "bert_f1": 0.8766641616821289,
        "meteor": 0.136986301369863,
        "sbert_similarity": 0.4064118266105652
      }
    },
    {
      "sample_id": 0,
      "question": "What did Caroline realize after her charity race?",
      "prediction": "Caroline realized that self-care is important for her mental health and family life.",
      "reference": "self-care is important",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.375,
        "rouge1_f": 0.4444444444444445,
        "rouge2_f": 0.375,
        "rougeL_f": 0.4444444444444445,
        "bleu1": 0.21428571428571425,
        "bleu2": 0.1815682598006407,
        "bleu3": 0.1428356262969865,
        "bleu4": 0.07069301148938888,
        "bert_precision": 0.8761155605316162,
        "bert_recall": 0.9327239394187927,
        "bert_f1": 0.903533935546875,
        "meteor": 0.7361111111111112,
        "sbert_similarity": 0.6308341026306152
      }
    },
    {
      "sample_id": 0,
      "question": "What are Melanie's plans for the summer with respect to adoption?",
      "prediction": "researching adoption agencies",
      "reference": "researching adoption agencies",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 0.5623413251903491,
        "bert_precision": 0.9999998807907104,
        "bert_recall": 0.9999998807907104,
        "bert_f1": 0.9999998807907104,
        "meteor": 0.9814814814814815,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 0,
      "question": "What type of individuals does the adoption agency Melanie is considering support?",
      "prediction": "LGBTQ+",
      "reference": "LGBTQ+ individuals",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.6666666666666666,
        "rouge1_f": 0.6666666666666666,
        "rouge2_f": 0.0,
        "rougeL_f": 0.6666666666666666,
        "bleu1": 0.36787944117144233,
        "bleu2": 0.11633369384516798,
        "bleu3": 0.08048325236348887,
        "bleu4": 0.06541924356118012,
        "bert_precision": 0.8917707204818726,
        "bert_recall": 0.8734214901924133,
        "bert_f1": 0.8825006484985352,
        "meteor": 0.2631578947368421,
        "sbert_similarity": 0.8889155983924866
      }
    },
    {
      "sample_id": 0,
      "question": "Why did Melanie choose the adoption agency?",
      "prediction": "Because the adoption agency supports LGBTQ+ individuals and emphasizes inclusivity and support, which aligns with Caroline's values.",
      "reference": "because of their inclusivity and support for LGBTQ+ individuals",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.4800000000000001,
        "rouge1_f": 0.4444444444444444,
        "rouge2_f": 0.24,
        "rougeL_f": 0.2962962962962963,
        "bleu1": 0.3,
        "bleu2": 0.21764287503300347,
        "bleu3": 0.14082229348267794,
        "bleu4": 0.0627251733901403,
        "bert_precision": 0.8874881267547607,
        "bert_recall": 0.9160969257354736,
        "bert_f1": 0.9015656113624573,
        "meteor": 0.5215419501133786,
        "sbert_similarity": 0.6568212509155273
      }
    },
    {
      "sample_id": 0,
      "question": "What is Melanie excited about in her adoption process?",
      "prediction": "a new family for kids who need one",
      "reference": "creating a family for kids who need one",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.875,
        "rouge1_f": 0.875,
        "rouge2_f": 0.7142857142857143,
        "rougeL_f": 0.875,
        "bleu1": 0.875,
        "bleu2": 0.7905694150420949,
        "bleu3": 0.7490836021568807,
        "bleu4": 0.7071067811865475,
        "bert_precision": 0.9703846573829651,
        "bert_recall": 0.9709939360618591,
        "bert_f1": 0.9706891775131226,
        "meteor": 0.864795918367347,
        "sbert_similarity": 0.8569421768188477
      }
    },
    {
      "sample_id": 0,
      "question": "What does Melanie's necklace symbolize?",
      "prediction": "love, faith and strength",
      "reference": "love, faith, and strength",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 0.8187307530779819,
        "bleu2": 0.7090416310250969,
        "bleu3": 0.5181569246973543,
        "bleu4": 0.2737591267534727,
        "bert_precision": 0.9906566739082336,
        "bert_recall": 0.9857161045074463,
        "bert_f1": 0.9881802201271057,
        "meteor": 0.6388888888888888,
        "sbert_similarity": 0.9917946457862854
      }
    },
    {
      "sample_id": 0,
      "question": "What country is Melanie's grandma from?",
      "prediction": "Not mentioned in the conversation or Sweden",
      "reference": "Sweden",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.25,
        "rouge1_f": 0.25,
        "rouge2_f": 0.0,
        "rougeL_f": 0.25,
        "bleu1": 0.14285714285714285,
        "bleu2": 0.048795003647426664,
        "bleu3": 0.037468614372419724,
        "bleu4": 0.033031643180138064,
        "bert_precision": 0.8157534599304199,
        "bert_recall": 0.8735286593437195,
        "bert_f1": 0.8436530232429504,
        "meteor": 0.31250000000000006,
        "sbert_similarity": 0.6158844828605652
      }
    },
    {
      "sample_id": 0,
      "question": "What was grandma's gift to Melanie?",
      "prediction": "necklace",
      "reference": "necklace",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 0.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 0.316227766016838,
        "bleu3": 0.2187761623949553,
        "bleu4": 0.1778279410038923,
        "bert_precision": 0.9999994039535522,
        "bert_recall": 0.9999994039535522,
        "bert_f1": 0.9999994039535522,
        "meteor": 0.5,
        "sbert_similarity": 0.9999999403953552
      }
    },
    {
      "sample_id": 0,
      "question": "What was grandpa's gift to Caroline?",
      "prediction": "necklace",
      "reference": "necklace",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 0.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 0.316227766016838,
        "bleu3": 0.2187761623949553,
        "bleu4": 0.1778279410038923,
        "bert_precision": 0.9999994039535522,
        "bert_recall": 0.9999994039535522,
        "bert_f1": 0.9999994039535522,
        "meteor": 0.5,
        "sbert_similarity": 0.9999999403953552
      }
    },
    {
      "sample_id": 0,
      "question": "What is Caroline's hand-painted bowl a reminder of?",
      "prediction": "art and self-expression",
      "reference": "art and self-expression",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 0.5623413251903491,
        "bert_precision": 0.9999998807907104,
        "bert_recall": 0.9999998807907104,
        "bert_f1": 0.9999998807907104,
        "meteor": 0.9814814814814815,
        "sbert_similarity": 1.0000001192092896
      }
    },
    {
      "sample_id": 0,
      "question": "What did Caroline and her family do while camping?",
      "prediction": "exploring nature, roasting marshmallows around the campfire and going on a hike.",
      "reference": "explored nature, roasted marshmallows, and went on a hike",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.5714285714285715,
        "rouge1_f": 0.761904761904762,
        "rouge2_f": 0.5263157894736842,
        "rougeL_f": 0.761904761904762,
        "bleu1": 0.5,
        "bleu2": 0.33968311024337877,
        "bleu3": 0.2159628241091146,
        "bleu4": 0.09669265690880861,
        "bert_precision": 0.9391342401504517,
        "bert_recall": 0.9620789885520935,
        "bert_f1": 0.9504681825637817,
        "meteor": 0.6048387096774195,
        "sbert_similarity": 0.7817373871803284
      }
    },
    {
      "sample_id": 0,
      "question": "What kind of counseling and mental health services is Melanie interested in pursuing?",
      "prediction": "counseling and mental health services that support self-acceptance and community building",
      "reference": "working with trans people, helping them accept themselves and supporting their mental health",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.2608695652173913,
        "rouge1_f": 0.4,
        "rouge2_f": 0.08695652173913043,
        "rougeL_f": 0.24000000000000002,
        "bleu1": 0.20762737819005644,
        "bleu2": 0.12572462101949167,
        "bleu3": 0.05253472342781006,
        "bleu4": 0.03358575137215658,
        "bert_precision": 0.8877458572387695,
        "bert_recall": 0.8759671449661255,
        "bert_f1": 0.8818172216415405,
        "meteor": 0.24658203125000006,
        "sbert_similarity": 0.4943845272064209
      }
    },
    {
      "sample_id": 0,
      "question": "What kind of counseling workshop did Melanie attend recently?",
      "prediction": "LGBTQ+ counseling workshop",
      "reference": "LGBTQ+ counseling workshop",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 0.5623413251903491,
        "bert_precision": 0.9999998807907104,
        "bert_recall": 0.9999998807907104,
        "bert_f1": 0.9999998807907104,
        "meteor": 0.9814814814814815,
        "sbert_similarity": 1.0000001192092896
      }
    },
    {
      "sample_id": 0,
      "question": "What motivated Melanie to pursue counseling?",
      "prediction": "her own journey and the support she received",
      "reference": "her own journey and the support she received, and how counseling improved her life",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.8,
        "rouge1_f": 0.7272727272727273,
        "rouge2_f": 0.7000000000000001,
        "rougeL_f": 0.7272727272727273,
        "bleu1": 0.4168620196785084,
        "bleu2": 0.4168620196785084,
        "bleu3": 0.4168620196785084,
        "bleu4": 0.4168620196785084,
        "bert_precision": 0.9820326566696167,
        "bert_recall": 0.9358775019645691,
        "bert_f1": 0.9583997130393982,
        "meteor": 0.47365214742613465,
        "sbert_similarity": 0.7598735094070435
      }
    },
    {
      "sample_id": 0,
      "question": "What kind of place does Melanie want to create for people?",
      "prediction": "art and creativity for people to grow, express themselves, and create meaningful experiences.",
      "reference": "a safe and inviting place for people to grow",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.4761904761904762,
        "rouge1_f": 0.4545454545454546,
        "rouge2_f": 0.3,
        "rougeL_f": 0.4545454545454546,
        "bleu1": 0.3125,
        "bleu2": 0.25,
        "bleu3": 0.2107453750519641,
        "bleu4": 0.16188613565728213,
        "bert_precision": 0.8937183618545532,
        "bert_recall": 0.8880277872085571,
        "bert_f1": 0.8908640146255493,
        "meteor": 0.39893617021276595,
        "sbert_similarity": 0.3696367144584656
      }
    },
    {
      "sample_id": 0,
      "question": "Did Caroline make the black and white bowl in the photo?",
      "prediction": "Not mentioned in the conversation or Yes",
      "reference": "Yes",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.25,
        "rouge1_f": 0.25,
        "rouge2_f": 0.0,
        "rougeL_f": 0.25,
        "bleu1": 0.14285714285714285,
        "bleu2": 0.048795003647426664,
        "bleu3": 0.037468614372419724,
        "bleu4": 0.033031643180138064,
        "bert_precision": 0.807045578956604,
        "bert_recall": 0.8341699838638306,
        "bert_f1": 0.820383608341217,
        "meteor": 0.31250000000000006,
        "sbert_similarity": 0.22492805123329163
      }
    },
    {
      "sample_id": 0,
      "question": "What are the new shoes that Caroline got used for?",
      "prediction": "Not mentioned in the conversation",
      "reference": "Running",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8501412272453308,
        "bert_recall": 0.8962051868438721,
        "bert_f1": 0.8725656270980835,
        "meteor": 0.0,
        "sbert_similarity": 0.16544058918952942
      }
    },
    {
      "sample_id": 0,
      "question": "What is Caroline's reason for getting into running?",
      "prediction": "To de-stress and clear her mind.",
      "reference": "To de-stress and clear her mind",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 0.8571428571428571,
        "bleu2": 0.8451542547285166,
        "bleu3": 0.8313759247878635,
        "bleu4": 0.8091067115702212,
        "bert_precision": 0.9813038110733032,
        "bert_recall": 0.9903073310852051,
        "bert_f1": 0.9857850074768066,
        "meteor": 0.8300000000000002,
        "sbert_similarity": 0.9632439017295837
      }
    },
    {
      "sample_id": 0,
      "question": "What does Caroline say running has been great for?",
      "prediction": "Caroline says running has been great for her mental health, inspiration, and connection with others.",
      "reference": "Her mental health",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.33333333333333337,
        "rouge1_f": 0.33333333333333337,
        "rouge2_f": 0.25,
        "rougeL_f": 0.33333333333333337,
        "bleu1": 0.16666666666666669,
        "bleu2": 0.140028008402801,
        "bleu3": 0.10943147363956583,
        "bleu4": 0.053463162573637084,
        "bert_precision": 0.8433928489685059,
        "bert_recall": 0.8887763023376465,
        "bert_f1": 0.8654900789260864,
        "meteor": 0.4464285714285714,
        "sbert_similarity": 0.4485304057598114
      }
    },
    {
      "sample_id": 0,
      "question": "What did Melanie see at the council meeting for adoption?",
      "prediction": "a council meeting for adoption",
      "reference": "many people wanting to create loving homes for children in need",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.12500000000000003,
        "rouge1_f": 0.12500000000000003,
        "rouge2_f": 0.0,
        "rougeL_f": 0.12500000000000003,
        "bleu1": 0.060238842382440407,
        "bleu2": 0.02129764696972561,
        "bleu3": 0.01706305179528683,
        "bleu4": 0.016182712188007015,
        "bert_precision": 0.8468752503395081,
        "bert_recall": 0.8351649045944214,
        "bert_f1": 0.8409793376922607,
        "meteor": 0.04807692307692308,
        "sbert_similarity": 0.4339033365249634
      }
    },
    {
      "sample_id": 0,
      "question": "What inspired Melanie's painting for the art show?",
      "prediction": "Not mentioned in the conversation.",
      "reference": "visiting an LGBTQ center and wanting to capture unity and strength",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8732311129570007,
        "bert_recall": 0.826892077922821,
        "bert_f1": 0.8494300842285156,
        "meteor": 0.0,
        "sbert_similarity": 0.014880775474011898
      }
    },
    {
      "sample_id": 0,
      "question": "What inspired Caroline's sculpture for the art show?",
      "prediction": "Not mentioned in the conversation.",
      "reference": "visiting an LGBTQ center and wanting to capture unity and strength",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8732311129570007,
        "bert_recall": 0.826892077922821,
        "bert_f1": 0.8494300842285156,
        "meteor": 0.0,
        "sbert_similarity": 0.014880775474011898
      }
    },
    {
      "sample_id": 0,
      "question": "How often does Caroline go to the beach with her kids?",
      "prediction": "once or twice a year",
      "reference": "once or twice a year",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 1.0,
        "bert_precision": 0.9999998807907104,
        "bert_recall": 0.9999998807907104,
        "bert_f1": 0.9999998807907104,
        "meteor": 0.996,
        "sbert_similarity": 0.9999999403953552
      }
    },
    {
      "sample_id": 0,
      "question": "What did Caroline and her family see during their camping trip last year?",
      "prediction": "nature and family time",
      "reference": "Perseid meteor shower",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8082737922668457,
        "bert_recall": 0.7670419216156006,
        "bert_f1": 0.7871182560920715,
        "meteor": 0.0,
        "sbert_similarity": 0.11349945515394211
      }
    },
    {
      "sample_id": 0,
      "question": "How did Caroline feel while watching the meteor shower?",
      "prediction": "Not mentioned in the conversation or in awe of the universe",
      "reference": "in awe of the universe",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.7142857142857143,
        "rouge1_f": 0.625,
        "rouge2_f": 0.5714285714285715,
        "rougeL_f": 0.625,
        "bleu1": 0.45454545454545453,
        "bleu2": 0.4264014327112209,
        "bleu3": 0.3964882289652318,
        "bleu4": 0.3508439695638686,
        "bert_precision": 0.8848599195480347,
        "bert_recall": 0.9345961213111877,
        "bert_f1": 0.9090481996536255,
        "meteor": 0.8892857142857143,
        "sbert_similarity": 0.7008298635482788
      }
    },
    {
      "sample_id": 0,
      "question": "Why did Caroline choose to use colors and patterns in her pottery project?",
      "prediction": "Not mentioned in the conversation",
      "reference": "She wanted to catch the eye and make people smile.",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.13333333333333333,
        "rouge1_f": 0.13333333333333333,
        "rouge2_f": 0.0,
        "rougeL_f": 0.13333333333333333,
        "bleu1": 0.060238842382440407,
        "bleu2": 0.02129764696972561,
        "bleu3": 0.01706305179528683,
        "bleu4": 0.016182712188007015,
        "bert_precision": 0.8825772404670715,
        "bert_recall": 0.8720124959945679,
        "bert_f1": 0.877263069152832,
        "meteor": 0.052631578947368425,
        "sbert_similarity": 0.04110643267631531
      }
    },
    {
      "sample_id": 0,
      "question": "Is Oscar Melanie's pet?",
      "prediction": "No",
      "reference": "Yes",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.999269425868988,
        "bert_recall": 0.999269425868988,
        "bert_f1": 0.999269425868988,
        "meteor": 0.0,
        "sbert_similarity": 0.7334951162338257
      }
    },
    {
      "sample_id": 0,
      "question": "Where did Oscar hide his bone once?",
      "prediction": "In Melanie's slipper",
      "reference": "In Melanie's slipper",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 1.0,
        "bert_precision": 1.000000238418579,
        "bert_recall": 1.000000238418579,
        "bert_f1": 1.000000238418579,
        "meteor": 0.9814814814814815,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 0,
      "question": "What activity did Melanie used to do with her dad?",
      "prediction": "Horseback riding",
      "reference": "Horseback riding",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 0.46773514128719823,
        "bleu4": 0.316227766016838,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.9375,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 0,
      "question": "What did Melanie make for a local church?",
      "prediction": "stained glass window",
      "reference": "a stained glass window",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.8571428571428571,
        "rouge1_f": 0.8571428571428571,
        "rouge2_f": 0.8,
        "rougeL_f": 0.8571428571428571,
        "bleu1": 0.7165313105737893,
        "bleu2": 0.7165313105737893,
        "bleu3": 0.7165313105737893,
        "bleu4": 0.40293516672844226,
        "bert_precision": 0.9519978761672974,
        "bert_recall": 0.9216736555099487,
        "bert_f1": 0.9365903735160828,
        "meteor": 0.754985754985755,
        "sbert_similarity": 0.9742969274520874
      }
    },
    {
      "sample_id": 0,
      "question": "What did Melanie find in her neighborhood during her walk?",
      "prediction": "Not mentioned in the conversation or a rainbow sidewalk",
      "reference": "a rainbow sidewalk",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.5,
        "rouge1_f": 0.5,
        "rouge2_f": 0.4,
        "rougeL_f": 0.5,
        "bleu1": 0.3333333333333333,
        "bleu2": 0.28867513459481287,
        "bleu3": 0.23173297416566233,
        "bleu4": 0.11868405219520975,
        "bert_precision": 0.8754580616950989,
        "bert_recall": 0.9470219612121582,
        "bert_f1": 0.9098349809646606,
        "meteor": 0.8179012345679013,
        "sbert_similarity": 0.7155337929725647
      }
    },
    {
      "sample_id": 0,
      "question": "Which song motivates Melanie to be courageous?",
      "prediction": "Not mentioned in the conversation",
      "reference": "Brave by Sara Bareilles",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8708803653717041,
        "bert_recall": 0.8365834951400757,
        "bert_f1": 0.8533874750137329,
        "meteor": 0.0,
        "sbert_similarity": -0.004309743642807007
      }
    },
    {
      "sample_id": 0,
      "question": "What type of instrument does Caroline play?",
      "prediction": "Not mentioned in the conversation or clarinet and violin",
      "reference": "clarinet and violin",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.5,
        "rouge1_f": 0.5,
        "rouge2_f": 0.4,
        "rougeL_f": 0.5,
        "bleu1": 0.3333333333333333,
        "bleu2": 0.28867513459481287,
        "bleu3": 0.23173297416566233,
        "bleu4": 0.11868405219520975,
        "bert_precision": 0.8562415838241577,
        "bert_recall": 0.8880078196525574,
        "bert_f1": 0.871835470199585,
        "meteor": 0.8179012345679013,
        "sbert_similarity": 0.7296750545501709
      }
    },
    {
      "sample_id": 0,
      "question": "Which classical musicians does Caroline enjoy listening to?",
      "prediction": "Bach and Mozart",
      "reference": "Bach and Mozart",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 0.5623413251903491,
        "bert_precision": 1.0000001192092896,
        "bert_recall": 1.0000001192092896,
        "bert_f1": 1.0000001192092896,
        "meteor": 0.9814814814814815,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 0,
      "question": "Who is Caroline a fan of in terms of modern music?",
      "prediction": "Ed Sheeran",
      "reference": "Ed Sheeran",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 0.46773514128719823,
        "bleu4": 0.316227766016838,
        "bert_precision": 0.9999997019767761,
        "bert_recall": 0.9999997019767761,
        "bert_f1": 0.9999997019767761,
        "meteor": 0.9375,
        "sbert_similarity": 1.0000001192092896
      }
    },
    {
      "sample_id": 0,
      "question": "What precautionary sign did Caroline see at the caf\u00e9?",
      "prediction": "Not mentioned in the conversation",
      "reference": "A sign stating that someone is not being able to leave",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.12500000000000003,
        "rouge1_f": 0.12500000000000003,
        "rouge2_f": 0.0,
        "rougeL_f": 0.12500000000000003,
        "bleu1": 0.060238842382440407,
        "bleu2": 0.02129764696972561,
        "bleu3": 0.01706305179528683,
        "bleu4": 0.016182712188007015,
        "bert_precision": 0.880416750907898,
        "bert_recall": 0.8307623863220215,
        "bert_f1": 0.8548691272735596,
        "meteor": 0.04807692307692308,
        "sbert_similarity": 0.09248798340559006
      }
    },
    {
      "sample_id": 0,
      "question": "What setback did Caroline face recently?",
      "prediction": "Not mentioned in the conversation",
      "reference": "She got hurt and had to take a break from pottery.",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8869463801383972,
        "bert_recall": 0.8621917963027954,
        "bert_f1": 0.8743939399719238,
        "meteor": 0.0,
        "sbert_similarity": 0.03774175047874451
      }
    },
    {
      "sample_id": 0,
      "question": "What does Caroline do to keep herself busy during her pottery break?",
      "prediction": "Read a book and paint.",
      "reference": "Read a book and paint.",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 1.0,
        "bert_precision": 0.9999999403953552,
        "bert_recall": 0.9999999403953552,
        "bert_f1": 0.9999999403953552,
        "meteor": 0.996,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 0,
      "question": "What was the poetry reading that Melanie attended about?",
      "prediction": "Not mentioned in the conversation or It was a transgender poetry reading where transgender people shared their stories.",
      "reference": "It was a transgender poetry reading where transgender people shared their stories.",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.7857142857142858,
        "rouge1_f": 0.8,
        "rouge2_f": 0.7857142857142858,
        "rougeL_f": 0.8,
        "bleu1": 0.6842105263157895,
        "bleu2": 0.6753816335059705,
        "bleu3": 0.6685192919093466,
        "bleu4": 0.6553609623522635,
        "bert_precision": 0.9195000529289246,
        "bert_recall": 0.9625145196914673,
        "bert_f1": 0.9405157566070557,
        "meteor": 0.9521053791887125,
        "sbert_similarity": 0.927978515625
      }
    },
    {
      "sample_id": 0,
      "question": "What happened to Caroline's son on their road trip?",
      "prediction": "He got into an accident.",
      "reference": "He got into an accident",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 0.8333333333333334,
        "bleu2": 0.816496580927726,
        "bleu3": 0.7955364837549187,
        "bleu4": 0.7598356856515925,
        "bert_precision": 0.9935369491577148,
        "bert_recall": 0.9922504425048828,
        "bert_f1": 0.9928932785987854,
        "meteor": 0.7937500000000002,
        "sbert_similarity": 0.9795498847961426
      }
    },
    {
      "sample_id": 0,
      "question": "How did Caroline's son handle the accident?",
      "prediction": "He was reassured by his family.",
      "reference": "He was scared but reassured by his family",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.8571428571428571,
        "rouge1_f": 0.8571428571428571,
        "rouge2_f": 0.6666666666666666,
        "rougeL_f": 0.8571428571428571,
        "bleu1": 0.7430381997858699,
        "bleu2": 0.6552980970848462,
        "bleu3": 0.5326417101825478,
        "bleu4": 0.42383656282787796,
        "bert_precision": 0.9769021272659302,
        "bert_recall": 0.9495986700057983,
        "bert_f1": 0.9630569219589233,
        "meteor": 0.6205128205128205,
        "sbert_similarity": 0.8808308839797974
      }
    },
    {
      "sample_id": 0,
      "question": "How did Caroline feel about her family after the accident?",
      "prediction": "They are important and mean the world to her",
      "reference": "They are important and mean the world to her",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 1.0,
        "bert_precision": 0.9999998211860657,
        "bert_recall": 0.9999998211860657,
        "bert_f1": 0.9999998211860657,
        "meteor": 0.9993141289437586,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 0,
      "question": "How did Caroline's children handle the accident?",
      "prediction": "They were scared but resilient.",
      "reference": "They were scared but resilient",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 0.8333333333333334,
        "bleu2": 0.816496580927726,
        "bleu3": 0.7955364837549187,
        "bleu4": 0.7598356856515925,
        "bert_precision": 0.9941954016685486,
        "bert_recall": 0.9930402040481567,
        "bert_f1": 0.9936175346374512,
        "meteor": 0.7937500000000002,
        "sbert_similarity": 0.9903228282928467
      }
    },
    {
      "sample_id": 0,
      "question": "How did Caroline feel after the accident?",
      "prediction": "Not mentioned in the conversation",
      "reference": "Grateful and thankful for her family",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8705781102180481,
        "bert_recall": 0.8196115493774414,
        "bert_f1": 0.844326376914978,
        "meteor": 0.0,
        "sbert_similarity": 0.01202389970421791
      }
    },
    {
      "sample_id": 0,
      "question": "What was Caroline's reaction to her children enjoying the Grand Canyon?",
      "prediction": "She was happy and thankful.",
      "reference": "She was happy and thankful",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 0.8333333333333334,
        "bleu2": 0.816496580927726,
        "bleu3": 0.7955364837549187,
        "bleu4": 0.7598356856515925,
        "bert_precision": 0.9946986436843872,
        "bert_recall": 0.9936434626579285,
        "bert_f1": 0.9941707849502563,
        "meteor": 0.7937500000000002,
        "sbert_similarity": 0.990138053894043
      }
    },
    {
      "sample_id": 0,
      "question": "What did Caroline do after the road trip to relax?",
      "prediction": "Went on a nature walk or hike",
      "reference": "Went on a nature walk or hike",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 1.0,
        "bert_precision": 0.9999998211860657,
        "bert_recall": 0.9999998211860657,
        "bert_f1": 0.9999998211860657,
        "meteor": 0.9985422740524781,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 0,
      "question": "What does Caroline love most about camping with her family?",
      "prediction": "Being present and bonding with her family",
      "reference": "Being present and bonding with her family",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 1.0,
        "bert_precision": 0.9999998211860657,
        "bert_recall": 0.9999998211860657,
        "bert_f1": 0.9999998211860657,
        "meteor": 0.9985422740524781,
        "sbert_similarity": 1.0000001192092896
      }
    }
  ]
}